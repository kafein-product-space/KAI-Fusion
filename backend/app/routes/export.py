# -*- coding: utf-8 -*-
"""Export functionality router."""

import logging
import uuid
import os
import tempfile
import zipfile
import json
from datetime import datetime
from typing import Dict, Any, List, Optional
from fastapi import APIRouter, HTTPException, status, Depends, File, UploadFile
from fastapi.responses import FileResponse
from pydantic import BaseModel, Field
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.workflow import Workflow
from app.services.workflow_service import WorkflowService
from app.auth.dependencies import get_current_user
from app.models.user import User
from app.core.database import get_db_session

logger = logging.getLogger(__name__)
router = APIRouter()

# ================================================================================
# WORKFLOW DOCKER EXPORT SCHEMAS
# ================================================================================

class WorkflowExportConfig(BaseModel):
    """Configuration for basic workflow export request."""
    include_credentials: bool = Field(default=False, description="Include credential schemas")
    export_format: str = Field(default="docker", description="Export format (docker, json)")

class EnvironmentVariable(BaseModel):
    """Environment variable definition."""
    name: str
    description: str
    example: Optional[str] = None
    default: Optional[str] = None
    required: bool = True
    node_type: Optional[str] = None

class WorkflowDependencies(BaseModel):
    """Workflow dependencies analysis result."""
    workflow_id: str
    nodes: List[str]
    required_env_vars: List[EnvironmentVariable]
    optional_env_vars: List[EnvironmentVariable]
    python_packages: List[str]
    api_endpoints: List[str]

class SecurityConfig(BaseModel):
    """Security configuration for exported workflow."""
    allowed_hosts: str = Field(default="*", description="Comma-separated allowed hosts (* for all hosts)")
    api_keys: Optional[str] = Field(default=None, description="Comma-separated API keys")
    require_api_key: bool = Field(default=False, description="Whether to require API key authentication")
    custom_api_keys: Optional[str] = Field(default=None, description="User custom API keys")

class MonitoringConfig(BaseModel):
    """Monitoring configuration for exported workflow."""
    enable_langsmith: bool = Field(default=False, description="Enable LangSmith monitoring")
    langsmith_api_key: Optional[str] = Field(default=None, description="LangSmith API key")
    langsmith_project: Optional[str] = Field(default=None, description="LangSmith project name")

class DockerConfig(BaseModel):
    """Docker configuration for exported workflow."""
    api_port: int = Field(default=8000, description="Internal API port")
    docker_port: int = Field(default=8000, description="External Docker port")
    database_url: Optional[str] = Field(default=None, description="External database URL")

class WorkflowEnvironmentConfig(BaseModel):
    """Complete environment configuration from user."""
    env_vars: Dict[str, str] = Field(default_factory=dict, description="Environment variables")
    security: SecurityConfig = Field(default_factory=SecurityConfig, description="Security settings")
    monitoring: MonitoringConfig = Field(default_factory=MonitoringConfig, description="Monitoring settings")
    docker: DockerConfig = Field(default_factory=DockerConfig, description="Docker settings")

class ExportPackage(BaseModel):
    """Export package information."""
    download_url: str
    package_size: int
    workflow_id: str
    export_timestamp: str
    ready_to_run: bool
    instructions: str

# ================================================================================
# NODE ENVIRONMENT MAPPING
# ================================================================================

NODE_ENV_MAPPING = {
    "OpenAIChat": {
        "required_env": ["OPENAI_API_KEY"],
        "optional_env": ["OPENAI_MODEL", "OPENAI_TEMPERATURE", "OPENAI_MAX_TOKENS"]
    },
    "TavilyWebSearch": {
        "required_env": ["TAVILY_API_KEY"],
        "optional_env": ["TAVILY_MAX_RESULTS", "TAVILY_SEARCH_DEPTH"]
    },
    "HttpClient": {
        "required_env": [],
        "optional_env": ["HTTP_TIMEOUT", "HTTP_MAX_RETRIES", "HTTP_USER_AGENT"]
    },
    "WebhookTrigger": {
        "required_env": [],
        "optional_env": ["WEBHOOK_SECRET", "WEBHOOK_TIMEOUT"]
    },
    "BufferMemory": {
        "required_env": [],
        "optional_env": ["MEMORY_BUFFER_SIZE", "MEMORY_RETURN_MESSAGES"]
    },
    "ConversationMemory": {
        "required_env": [],
        "optional_env": ["CONVERSATION_MEMORY_K", "MEMORY_KEY"]
    },
    "CohereReranker": {
        "required_env": ["COHERE_API_KEY"],
        "optional_env": ["COHERE_MODEL", "COHERE_TOP_K"]
    },
    "VectorStoreOrchestrator": {
        "required_env": ["DATABASE_URL"],
        "optional_env": ["VECTOR_STORE_COLLECTION", "EMBEDDING_DIMENSIONS"]
    },
    "DocumentLoader": {
        "required_env": [],
        "optional_env": ["DOC_LOADER_CHUNK_SIZE", "DOC_LOADER_OVERLAP"]
    },
    "WebScraper": {
        "required_env": [],
        "optional_env": ["SCRAPER_TIMEOUT", "SCRAPER_USER_AGENT"]
    }
}

ENV_DESCRIPTIONS = {
    "OPENAI_API_KEY": "OpenAI API key for LLM operations",
    "OPENAI_MODEL": "OpenAI model name (default: gpt-3.5-turbo)",
    "OPENAI_TEMPERATURE": "Temperature for AI responses (0.0-2.0)",
    "OPENAI_MAX_TOKENS": "Maximum tokens for AI responses",
    "TAVILY_API_KEY": "Tavily API key for web search",
    "TAVILY_MAX_RESULTS": "Maximum search results to return",
    "TAVILY_SEARCH_DEPTH": "Search depth (basic/advanced)",
    "HTTP_TIMEOUT": "HTTP request timeout in seconds",
    "HTTP_MAX_RETRIES": "Maximum HTTP retry attempts",
    "HTTP_USER_AGENT": "Custom User-Agent for HTTP requests",
    "WEBHOOK_SECRET": "Secret for webhook signature validation",
    "WEBHOOK_TIMEOUT": "Webhook timeout in seconds",
    "MEMORY_BUFFER_SIZE": "Buffer memory size limit",
    "MEMORY_RETURN_MESSAGES": "Return messages format (true/false)",
    "CONVERSATION_MEMORY_K": "Number of messages to keep in memory",
    "MEMORY_KEY": "Memory key for conversation storage",
    "COHERE_API_KEY": "Cohere API key for reranking",
    "COHERE_MODEL": "Cohere model for reranking",
    "COHERE_TOP_K": "Top K results to rerank",
    "DATABASE_URL": "PostgreSQL database connection URL",
    "VECTOR_STORE_COLLECTION": "Vector store collection name",
    "EMBEDDING_DIMENSIONS": "Embedding vector dimensions",
    "DOC_LOADER_CHUNK_SIZE": "Document chunk size for processing",
    "DOC_LOADER_OVERLAP": "Chunk overlap size",
    "SCRAPER_TIMEOUT": "Web scraper timeout in seconds",
    "SCRAPER_USER_AGENT": "User agent for web scraping"
}

ENV_EXAMPLES = {
    "OPENAI_API_KEY": "sk-1234567890abcdef...",
    "OPENAI_MODEL": "gpt-4",
    "OPENAI_TEMPERATURE": "0.7",
    "OPENAI_MAX_TOKENS": "2048",
    "TAVILY_API_KEY": "tvly-1234567890abcdef...",
    "TAVILY_MAX_RESULTS": "5",
    "TAVILY_SEARCH_DEPTH": "advanced",
    "HTTP_TIMEOUT": "30",
    "HTTP_MAX_RETRIES": "3",
    "HTTP_USER_AGENT": "KAI-Fusion-Workflow/1.0",
    "WEBHOOK_SECRET": "your-webhook-secret",
    "WEBHOOK_TIMEOUT": "30",
    "MEMORY_BUFFER_SIZE": "1000",
    "MEMORY_RETURN_MESSAGES": "true",
    "CONVERSATION_MEMORY_K": "5",
    "MEMORY_KEY": "chat_history",
    "COHERE_API_KEY": "cohere-key-1234...",
    "COHERE_MODEL": "rerank-english-v2.0",
    "COHERE_TOP_K": "10",
    "DATABASE_URL": "postgresql://user:pass@localhost:5432/dbname",
    "VECTOR_STORE_COLLECTION": "documents",
    "EMBEDDING_DIMENSIONS": "1536",
    "DOC_LOADER_CHUNK_SIZE": "1000",
    "DOC_LOADER_OVERLAP": "200",
    "SCRAPER_TIMEOUT": "30",
    "SCRAPER_USER_AGENT": "Mozilla/5.0..."
}

ENV_DEFAULTS = {
    "OPENAI_MODEL": "gpt-3.5-turbo",
    "OPENAI_TEMPERATURE": "0.7",
    "OPENAI_MAX_TOKENS": "2048",
    "TAVILY_MAX_RESULTS": "5",
    "TAVILY_SEARCH_DEPTH": "basic",
    "HTTP_TIMEOUT": "30",
    "HTTP_MAX_RETRIES": "3",
    "HTTP_USER_AGENT": "KAI-Fusion-Workflow/1.0",
    "WEBHOOK_TIMEOUT": "30",
    "MEMORY_BUFFER_SIZE": "1000",
    "MEMORY_RETURN_MESSAGES": "true",
    "CONVERSATION_MEMORY_K": "5",
    "MEMORY_KEY": "chat_history",
    "COHERE_MODEL": "rerank-english-v2.0",
    "COHERE_TOP_K": "10",
    "VECTOR_STORE_COLLECTION": "documents",
    "EMBEDDING_DIMENSIONS": "1536",
    "DOC_LOADER_CHUNK_SIZE": "1000",
    "DOC_LOADER_OVERLAP": "200",
    "SCRAPER_TIMEOUT": "30",
    "SCRAPER_USER_AGENT": "Mozilla/5.0 (compatible; KAI-Fusion-WebScraper/1.0)"
}

# ================================================================================
# HELPER FUNCTIONS  
# ================================================================================

def analyze_workflow_dependencies(flow_data: Dict[str, Any]) -> WorkflowDependencies:
    """Analyze workflow and extract dependencies, including node credentials."""
    logger.info("Analyzing workflow dependencies")
    
    # Extract nodes from flow_data
    nodes = flow_data.get("nodes", [])
    node_types = []
    node_credentials = {}  # Store found credentials
    
    for node in nodes:
        node_type = node.get("type", "")
        if node_type and node_type not in node_types:
            node_types.append(node_type)
            
        # Extract credentials from node data
        node_id = node.get("id", "")
        node_data = node.get("data", {})
        
        if node_data:
            # Look for credential fields in node data
            credentials = {}
            for key, value in node_data.items():
                if any(cred_key in key.lower() for cred_key in ['api_key', 'token', 'secret', 'password', 'key']):
                    if value:  # Only include if credential is set
                        credentials[key] = value
            
            if credentials:
                node_credentials[node_id] = {
                    "type": node_type,
                    "credentials": credentials
                }
    
    logger.info(f"Found node types: {node_types}")
    logger.info(f"Found credentials in {len(node_credentials)} nodes")
    
    # Only collect credential environment variables (API keys and secrets)
    required_env_vars = []
    optional_env_vars = []
    python_packages = ["fastapi", "uvicorn", "sqlalchemy", "asyncpg", "pydantic"]
    
    # Always require DATABASE_URL for workflow execution data
    required_env_vars.append(EnvironmentVariable(
        name="DATABASE_URL",
        description="Database connection URL for storing workflow execution data",
        example="postgresql://user:password@localhost:5432/workflow_db",
        required=True,
        node_type="System"
    ))
    
    # Add environment variables for node credentials found in workflow
    for node_id, node_info in node_credentials.items():
        node_type = node_info["type"]
        credentials = node_info["credentials"]
        
        for cred_key, cred_value in credentials.items():
            # Create environment variable names from credential keys
            env_var_name = f"{node_type.upper()}_{cred_key.upper()}"
            
            # Only add if not already exists
            if env_var_name not in [v.name for v in required_env_vars]:
                required_env_vars.append(EnvironmentVariable(
                    name=env_var_name,
                    description=f"{cred_key} for {node_type} node ({node_id})",
                    example=str(cred_value) if len(str(cred_value)) < 20 else f"{str(cred_value)[:15]}...",
                    required=True,
                    node_type=f"{node_type} ({node_id})"
                ))
    
    # Define backup credential environment variables if no node credentials found
    credential_mapping = {
        "OpenAI": ["OPENAI_API_KEY"],
        "ChatOpenAI": ["OPENAI_API_KEY"],
        "TavilyWebSearch": ["TAVILY_API_KEY"],
        "Tavily": ["TAVILY_API_KEY"],
        "CohereReranker": ["COHERE_API_KEY"],
        "Cohere": ["COHERE_API_KEY"]
    }
    
    # Add standard credential requirements for nodes without specific credentials
    for node_type in node_types:
        if node_type in credential_mapping:
            for credential_var in credential_mapping[node_type]:
                if credential_var not in [v.name for v in required_env_vars]:
                    # Check if this node type already has credentials from workflow
                    has_node_credentials = any(
                        info["type"] == node_type for info in node_credentials.values()
                    )
                    
                    if not has_node_credentials:
                        required_env_vars.append(EnvironmentVariable(
                            name=credential_var,
                            description=ENV_DESCRIPTIONS.get(credential_var, f"API key for {node_type}"),
                            example=ENV_EXAMPLES.get(credential_var, ""),
                            required=True,
                            node_type=node_type
                        ))
    
    # Optional: LangSmith monitoring API key
    optional_env_vars.append(EnvironmentVariable(
        name="LANGCHAIN_API_KEY",
        description="LangSmith API key for monitoring (optional)",
        example="lsv2_sk_abc123...",
        default="",
        required=False,
        node_type="Monitoring"
    ))
    
    # Add node-specific packages based on found node types
    for node_type in node_types:
        if node_type in ["OpenAI", "ChatOpenAI"]:
            python_packages.extend(["langchain-openai", "openai"])
        elif node_type == "TavilyWebSearch":
            python_packages.extend(["langchain-tavily"])
        elif node_type == "HttpClient":
            python_packages.extend(["httpx", "requests"])
        elif node_type == "CohereReranker":
            python_packages.extend(["cohere", "langchain-cohere"])
        elif node_type == "VectorStoreOrchestrator":
            python_packages.extend(["pgvector", "langchain-postgres"])
        elif "Memory" in node_type:
            python_packages.extend(["langchain"])
    
    # Remove duplicates
    python_packages = list(set(python_packages))
    
    # Define API endpoints
    api_endpoints = [
        "POST /api/workflow/execute",
        "GET /api/workflow/status/{execution_id}",
        "GET /api/workflow/result/{execution_id}",
        "GET /api/health",
        "GET /api/workflow/info",
        "GET /api/workflow/external/info",
        "POST /api/workflow/external/ping",
        "GET /api/workflow/external/metrics"
    ]
    
    return WorkflowDependencies(
        workflow_id="temp_id",
        nodes=node_types,
        required_env_vars=required_env_vars,
        optional_env_vars=optional_env_vars,
        python_packages=python_packages,
        api_endpoints=api_endpoints
    )

def get_required_env_vars_for_workflow(dependencies: WorkflowDependencies) -> Dict[str, Any]:
    """Get required environment variables for workflow."""
    return {
        "required": [
            {
                "name": var.name,
                "description": var.description,
                "example": var.example,
                "node": var.node_type
            }
            for var in dependencies.required_env_vars
        ],
        "optional": [
            {
                "name": var.name,
                "description": var.description,
                "default": var.default,
                "node": var.node_type
            }
            for var in dependencies.optional_env_vars
        ]
    }

# ================================================================================
# WORKFLOW DOCKER EXPORT ENDPOINTS
# ================================================================================

@router.post("/export/workflow/{workflow_id}", tags=["Export"])
async def export_workflow(
    workflow_id: str,
    export_config: WorkflowExportConfig,
    db: AsyncSession = Depends(get_db_session),
    current_user: User = Depends(get_current_user),
    workflow_service: WorkflowService = Depends()
):
    """
    Initialize workflow export and return required environment variables.
    
    This endpoint analyzes the workflow and returns the required environment
    variables that need to be provided by the user to complete the export.
    """
    try:
        logger.info(f"Starting workflow export for workflow_id: {workflow_id}")
        
        # Get workflow
        workflow = await workflow_service.get_by_id(
            db, uuid.UUID(workflow_id), current_user.id
        )
        
        if not workflow:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Workflow not found or access denied"
            )
        
        # Analyze dependencies
        dependencies = analyze_workflow_dependencies(workflow.flow_data)
        dependencies.workflow_id = workflow_id
        
        # Get required environment variables
        required_env_vars = get_required_env_vars_for_workflow(dependencies)
        
        logger.info(f"Workflow analysis complete. Required vars: {len(required_env_vars['required'])}")
        
        return {
            "workflow_id": workflow_id,
            "workflow_name": workflow.name,
            "workflow_description": workflow.description,
            "required_env_vars": required_env_vars,
            "dependencies": {
                "nodes": dependencies.nodes,
                "python_packages": dependencies.python_packages,
                "api_endpoints": dependencies.api_endpoints
            },
            "export_ready": False,
            "message": "Please provide environment variables to complete export"
        }
        
    except ValueError as e:
        logger.error(f"Invalid workflow ID: {e}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid workflow ID format"
        )
    except Exception as e:
        logger.error(f"Workflow export initialization failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to initialize workflow export"
        )

@router.post("/export/workflow/{workflow_id}/complete", tags=["Export"])
async def complete_workflow_export(
    workflow_id: str,
    env_config: WorkflowEnvironmentConfig,
    db: AsyncSession = Depends(get_db_session),
    current_user: User = Depends(get_current_user),
    workflow_service: WorkflowService = Depends()
):
    """
    Complete workflow export with user-provided environment variables.
    
    This endpoint creates a ready-to-run Docker package with the user's
    environment configuration, security settings, and monitoring options.
    """
    try:
        logger.info(f"Completing workflow export for workflow_id: {workflow_id}")
        
        # Get workflow
        workflow = await workflow_service.get_by_id(
            db, uuid.UUID(workflow_id), current_user.id
        )
        
        if not workflow:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Workflow not found or access denied"
            )
        
        # Re-analyze dependencies
        dependencies = analyze_workflow_dependencies(workflow.flow_data)
        dependencies.workflow_id = workflow_id
        
        # Validate environment variables
        validation_result = validate_env_variables(dependencies, env_config.env_vars)
        if not validation_result["valid"]:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Invalid environment variables: {validation_result['errors']}"
            )
        
        # Create minimal backend components
        minimal_backend = create_minimal_backend(dependencies)
        
        # Filter requirements for nodes
        filtered_requirements = filter_requirements_for_nodes(dependencies.nodes)
        
        # Create pre-configured .env file
        pre_configured_env = create_pre_configured_env_file(
            dependencies, env_config.env_vars, env_config.security, 
            env_config.monitoring, env_config.docker, workflow.flow_data
        )
        
        # Create Docker context
        docker_context = create_ready_to_run_docker_context(
            workflow, minimal_backend, pre_configured_env, env_config.docker
        )
        
        # Create export package
        export_package = create_workflow_export_package({
            'workflow': workflow,
            'backend': minimal_backend,
            'pre_configured_env': pre_configured_env,
            'docker_configs': docker_context,
            'filtered_requirements': filtered_requirements,
            'readme': generate_ready_to_run_readme(workflow.name, env_config)
        })
        
        logger.info(f"Export package created: {export_package['download_url']}")
        
        return {
            "workflow_id": workflow_id,
            "download_url": export_package["download_url"],
            "package_size": export_package["package_size"],
            "ready_to_run": True,
            "export_timestamp": datetime.now().isoformat(),
            "instructions": "Extract and run: docker-compose up -d",
            "package_info": {
                "workflow_name": workflow.name,
                "included_nodes": dependencies.nodes,
                "api_port": env_config.docker.docker_port,
                "security_enabled": env_config.security.require_api_key,
                "monitoring_enabled": env_config.monitoring.enable_langsmith
            }
        }
        
    except ValueError as e:
        logger.error(f"Invalid workflow ID: {e}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Invalid workflow ID format"
        )
    except Exception as e:
        logger.error(f"Workflow export completion failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to complete workflow export"
        )

def validate_env_variables(dependencies: WorkflowDependencies, env_vars: Dict[str, str]) -> Dict[str, Any]:
    """Validate provided environment variables against requirements."""
    errors = []
    warnings = []
    
    # Check required variables
    for var in dependencies.required_env_vars:
        if var.name not in env_vars or not env_vars[var.name].strip():
            errors.append(f"Required environment variable '{var.name}' is missing or empty")
    
    # Check for potentially invalid values
    for var_name, var_value in env_vars.items():
        if var_name == "OPENAI_API_KEY" and not var_value.startswith("sk-"):
            warnings.append(f"OpenAI API key format may be invalid (should start with 'sk-')")
        elif var_name == "TAVILY_API_KEY" and not var_value.startswith("tvly-"):
            warnings.append(f"Tavily API key format may be invalid (should start with 'tvly-')")
        elif var_name == "DATABASE_URL" and not var_value.startswith(("postgresql://", "mysql://", "sqlite://")):
            warnings.append(f"Database URL format may be invalid")
    
    return {
        "valid": len(errors) == 0,
        "errors": errors,
        "warnings": warnings
    }

def create_minimal_backend(dependencies: WorkflowDependencies) -> Dict[str, str]:
    """Create minimal backend components for workflow runtime."""
    logger.info("Creating minimal backend components")
    
    # Main application file
    main_py = f'''# -*- coding: utf-8 -*-
"""
Minimal KAI-Fusion Workflow Runtime
Auto-generated runtime for workflow: {dependencies.workflow_id}
"""

import os
import logging
from datetime import datetime
from typing import Dict, Any, Optional
from fastapi import FastAPI, HTTPException, Header, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import json
import uuid

# Import modular components
from app.api.workflow import router as workflow_router
from app.api.health import router as health_router
from app.core.config import settings
from app.core.database import init_database, close_database

# Configure logging
logging.basicConfig(
    level=getattr(logging, os.getenv('LOG_LEVEL', 'INFO')),
    format=os.getenv('LOG_FORMAT', '%(asctime)s - %(name)s - %(levelname)s - %(message)s')
)
logger = logging.getLogger(__name__)

# Load workflow definition
try:
    with open('workflow-definition.json', 'r', encoding='utf-8') as f:
        WORKFLOW_DEFINITION = json.load(f)
    logger.info("Workflow definition loaded successfully")
except Exception as e:
    logger.error(f"Failed to load workflow definition: {{e}}")
    WORKFLOW_DEFINITION = {{"nodes": [], "edges": []}}

# Security configuration
API_KEYS = [key.strip() for key in (settings.API_KEYS or '').split(',') if key.strip()]
# Use REQUIRE_API_KEY setting from export configuration
REQUIRE_API_KEY = settings.REQUIRE_API_KEY.lower() == 'true'

# Minimal startup log (avoid template-time f-string evaluation)
logger.info("Workflow runtime security configuration loaded")

# Initialize FastAPI
app = FastAPI(
    title="KAI-Fusion Workflow Runtime",
    description=f"Runtime for workflow: {{settings.WORKFLOW_ID}}",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins for workflow exports
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request models
class WorkflowExecuteRequest(BaseModel):
    input: str
    parameters: Dict[str, Any] = {{}}

class WorkflowExecuteResponse(BaseModel):
    execution_id: str
    status: str
    result: Optional[Any] = None
    error: Optional[str] = None
    timestamp: str

# Security middleware
@app.middleware("http")
async def security_middleware(request: Request, call_next):
    # API key validation only (no host validation as requested)
    if REQUIRE_API_KEY:
        path = request.url.path
        if path.startswith("/api/") and not path.startswith("/api/health"):
            api_key = request.headers.get("X-API-Key")
            auth_header = request.headers.get("Authorization")

            if not api_key and auth_header and auth_header.startswith("Bearer "):
                api_key = auth_header.split(" ")[1]

            if not api_key:
                logger.warning("API key is missing")
                raise HTTPException(status_code=401, detail="API key required")
            if api_key not in API_KEYS:
                logger.warning("Invalid API key provided")
                raise HTTPException(status_code=401, detail="Invalid API key")
            logger.info("Request authenticated successfully")

    response = await call_next(request)
    return response

# Include API routers
app.include_router(health_router, prefix="/api/health", tags=["Health"])
app.include_router(workflow_router, prefix="/api/workflow", tags=["Workflow"])

# Root endpoint
@app.get("/")
async def root():
    return {{
        "service": "KAI-Fusion Workflow Runtime",
        "version": "1.0.0", 
        "workflow_id": settings.WORKFLOW_ID,
        "status": "running",
        "docs": "/docs",
        "api_endpoints": {{
            "health": "/api/health",
            "workflow": "/api/workflow",
            "execute": "/api/workflow/execute"
        }}
    }}

# Legacy health endpoint for backwards compatibility 
@app.get("/health")
async def legacy_health():
    return {{
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "workflow_id": settings.WORKFLOW_ID
    }}

# Application lifecycle events
@app.on_event("startup")
async def startup_event():
    """Initialize database and services on startup."""
    try:
        await init_database()
        logger.info("Workflow runtime started successfully")
    except Exception as e:
        logger.error(f"Startup failed: {{e}}")
        raise

@app.on_event("shutdown")
async def shutdown_event():
    """Clean up resources on shutdown."""
    try:
        await close_database()
        logger.info("Workflow runtime stopped")
    except Exception as e:
        logger.error(f"Shutdown error: {{e}}")

# Placeholder external workflow integration
@app.get("/api/workflow/external/info")
async def get_external_workflow_info():
    return {{
        "workflow_id": settings.WORKFLOW_ID,
        "name": WORKFLOW_DEFINITION.get("name", "External Workflow"),
        "description": WORKFLOW_DEFINITION.get("description", "Exported workflow running in Docker"),
        "version": "1.0.0",
        "is_external": True,
        "external_host": os.getenv('EXTERNAL_HOST', ''),
        "flow_data": WORKFLOW_DEFINITION,
        "capabilities": {{
            "execution": True,
            "monitoring": bool(os.getenv("LANGCHAIN_TRACING_V2")),
            "api_key_required": REQUIRE_API_KEY
        }},
        "connection_info": {{
            "last_ping": datetime.now().isoformat(),
            "uptime": "healthy",
            "health": "healthy"
        }}
    }}

@app.post("/api/workflow/external/ping")
async def external_ping():
    return {{
        "status": "alive",
        "timestamp": datetime.now().isoformat(),
        "workflow_id": settings.WORKFLOW_ID,
        "uptime": "healthy"
    }}

# Workflow execution (placeholder - would integrate with actual engine)
@app.post("/api/workflow/execute", response_model=WorkflowExecuteResponse)
async def execute_workflow(request: WorkflowExecuteRequest):
    execution_id = str(uuid.uuid4())
    
    try:
        # TODO: Integrate with actual workflow engine
        # This is a placeholder implementation
        result = {{
            "message": "Workflow execution completed",
            "input": request.input,
            "parameters": request.parameters,
            "processed_at": datetime.now().isoformat()
        }}
        
        return WorkflowExecuteResponse(
            execution_id=execution_id,
            status="completed",
            result=result,
            timestamp=datetime.now().isoformat()
        )
    
    except Exception as e:
        logger.error(f"Workflow execution failed: {{e}}")
        return WorkflowExecuteResponse(
            execution_id=execution_id,
            status="failed",
            error=str(e),
            timestamp=datetime.now().isoformat()
        )

@app.get("/api/workflow/status/{{execution_id}}")
async def get_execution_status(execution_id: str):
    # TODO: Implement actual status tracking
    return {{
        "execution_id": execution_id,
        "status": "completed",
        "timestamp": datetime.now().isoformat()
    }}

@app.get("/api/workflow/result/{{execution_id}}")
async def get_execution_result(execution_id: str):
    # TODO: Implement actual result retrieval
    return {{
        "execution_id": execution_id,
        "result": "Placeholder result",
        "timestamp": datetime.now().isoformat()
    }}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        app,
        host=settings.HOST,
        port=settings.API_PORT
    )
'''
    
    # Dockerfile
    dockerfile = f'''FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \\
    libpq-dev \\
    curl \\
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies  
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt && \
    pip install --no-cache-dir uvicorn[standard]>=0.24.0 sqlalchemy>=2.0.0 asyncpg>=0.28.0

# Copy application files
COPY main.py .
COPY app/ ./app/
COPY workflow-definition.json .
COPY .env .

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\
    CMD curl -f http://localhost:8000/health || exit 1

# Create logs directory with proper permissions
RUN mkdir -p /app/logs && \\
    chmod -R 755 /app && \\
    chown -R root:root /app

EXPOSE 8000

# Create startup script to ensure uvicorn is available
RUN echo '#!/bin/bash' > /start.sh && \\
    echo 'echo "Checking required packages..."' >> /start.sh && \\
    echo 'python -m pip show uvicorn || pip install uvicorn[standard]' >> /start.sh && \\
    echo 'python -m pip show asyncpg || pip install asyncpg' >> /start.sh && \\
    echo 'python -m pip show sqlalchemy || pip install sqlalchemy' >> /start.sh && \\
    echo 'echo "Starting application..."' >> /start.sh && \\
    echo 'cd /app' >> /start.sh && \\
    echo 'exec python -m uvicorn main:app --host 0.0.0.0 --port $API_PORT --log-level info' >> /start.sh && \\
    chmod +x /start.sh

# Start application
CMD ["/start.sh"]
'''
    
    return {
        "main.py": main_py,
        "Dockerfile": dockerfile
    }

def filter_requirements_for_nodes(node_types: List[str]) -> str:
    """Filter requirements.txt for only needed packages."""
    logger.info(f"Filtering requirements for nodes: {node_types}")
    
    # Copy all packages from main requirements.txt
    base_packages = [
        "fastapi>=0.104.0",
        "uvicorn[standard]>=0.24.0",
        "sqlalchemy>=2.0.0",
        "asyncpg>=0.28.0",
        "alembic>=1.12.0",
        "psycopg2-binary>=2.9.0",
        "PyJWT>=2.8.0",
        "pgvector>=0.2.0",
        "pydantic>=2.5.0",
        "pydantic-settings>=2.1.0",
        "email-validator>=2.1.0",
        "python-jose[cryptography]>=3.3.0",
        "passlib==1.7.4",
        "bcrypt<4.1.0",
        "cryptography>=41.0.0",
        "python-dotenv>=1.0.0",
        "httpx>=0.25.0",
        "requests>=2.31.0",
        "jinja2>=3.1.0",
        "psutil>=5.9.0",
        "python-multipart>=0.0.6",
        "beautifulsoup4>=4.12.0",
        "lxml>=4.9.0",
        "numpy>=1.24.0",
        "scikit-learn>=1.3.0"
    ]
    
    node_packages = []
    
    # Add common langchain packages for any workflow
    common_langchain_packages = [
        "langchain>=0.1.0",
        "langchain-core>=0.1.0",
        "langchain-community>=0.0.10",
        "langchain-text-splitters>=0.3.0",
        "langgraph>=0.0.30",
        "openai>=1.0.0"  # Most workflows use OpenAI
    ]
    node_packages.extend(common_langchain_packages)
    
    for node_type in node_types:
        if node_type == "OpenAIChat":
            node_packages.extend([
                "langchain-openai>=0.0.5",
                "openai>=1.0.0"
            ])
        elif node_type == "TavilyWebSearch":
            node_packages.extend([
                "langchain-tavily>=0.2.0"
            ])
        elif node_type == "CohereReranker":
            node_packages.extend([
                "cohere==5.12.0",
                "langchain-cohere>=0.4.0",
                "rank-bm25>=0.2.2"
            ])
        elif node_type == "VectorStoreOrchestrator":
            node_packages.extend([
                "langchain-postgres>=0.0.15"
            ])
        elif node_type in ["BufferMemory", "ConversationMemory"]:
            # Already included in common langchain packages
            pass
    
    # Remove duplicates and combine
    all_packages = list(set(base_packages + node_packages))
    return "\n".join(sorted(all_packages))

def create_pre_configured_env_file(
    dependencies: WorkflowDependencies, 
    user_env_vars: Dict[str, str],
    security: SecurityConfig,
    monitoring: MonitoringConfig,
    docker: DockerConfig,
    flow_data: Dict[str, Any]
) -> str:
    """Create pre-configured .env file with user settings."""
    logger.info("Creating pre-configured .env file")
    
    env_content = [
        "# Generated .env file for workflow export",
        "# Ready to run - no additional configuration needed",
        f"# Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        "",
        "# Workflow Configuration",
        f"WORKFLOW_ID={dependencies.workflow_id}",
        f"WORKFLOW_MODE=runtime",
        "",
        "# External Database Configuration", 
        f"DATABASE_URL={user_env_vars.get('DATABASE_URL', 'postgresql://user:pass@localhost:5432/workflow_db')}",
        "",
        "# Security & Authentication",
        f"SECRET_KEY={user_env_vars.get('SECRET_KEY', 'auto-generated-secret-key-' + str(uuid.uuid4()))}",
        f"REQUIRE_API_KEY={str(security.require_api_key).lower()}",
        "",
    ]
    
    # API Keys (combine system and custom keys)
    api_keys = []
    if security.api_keys:
        api_keys.extend([key.strip() for key in security.api_keys.split(',') if key.strip()])
    if security.custom_api_keys:
        api_keys.extend([key.strip() for key in security.custom_api_keys.split(',') if key.strip()])
    
    # Use the exact setting from export configuration (no auto-enable)
    # User explicitly chooses whether API key is required or not
    
    if api_keys:
        env_content.append(f"API_KEYS={','.join(api_keys)}")
    else:
        env_content.append("API_KEYS=")
    
    env_content.extend([
        "",
        "# LangSmith Monitoring (Optional)",
        f"LANGCHAIN_TRACING_V2={str(monitoring.enable_langsmith).lower()}",
        f"LANGCHAIN_API_KEY={monitoring.langsmith_api_key or ''}",
        f"LANGCHAIN_PROJECT={monitoring.langsmith_project or dependencies.workflow_id}",
        "",
        "# Runtime Application",
        "API_HOST=0.0.0.0", 
        f"API_PORT={docker.api_port}",
        f"DOCKER_PORT={docker.docker_port}",
        "",
        "# Auto-migration settings",
        "AUTO_MIGRATE=true",
        "MIGRATION_TIMEOUT=30",
        ""
    ])
    
    # Collect all environment variables to avoid duplicates
    used_env_vars = set(["DATABASE_URL", "SECRET_KEY", "WORKFLOW_ID", "WORKFLOW_MODE", 
                        "REQUIRE_API_KEY", "API_KEYS",
                        "LANGCHAIN_TRACING_V2", "LANGCHAIN_API_KEY", "LANGCHAIN_PROJECT",
                        "API_HOST", "API_PORT", "DOCKER_PORT", "AUTO_MIGRATE", "MIGRATION_TIMEOUT"])
    
    # API Keys and Credentials (from user input - not from nodes)
    credential_vars = {}
    for env_var, value in user_env_vars.items():
        if env_var not in used_env_vars:
            # Check if this is a node-specific variable
            if any(node_type in env_var for node_type in ["OPENAI", "TAVILY", "COHERE", "LANGCHAIN"]):
                continue  # Skip node-specific vars, they'll be handled in node sections
            credential_vars[env_var] = value
            used_env_vars.add(env_var)
    
    if credential_vars:
        env_content.append("# API Keys and Credentials")
        for env_var, value in credential_vars.items():
            env_content.append(f"{env_var}={value}")
        env_content.append("")
    
    # Node Configuration Variables (organized by node type)
    nodes = flow_data.get("nodes", [])
    node_sections = {}
    
    # Group nodes by type
    for node in nodes:
        node_id = node.get("id", "")
        node_type = node.get("type", "")
        node_data = node.get("data", {})
        
        if node_data and node_type:
            if node_type not in node_sections:
                node_sections[node_type] = []
            
            # Collect configuration for this node
            node_config = {}
            
            # Add common settings
            for key, value in node_data.items():
                if key in ["model", "temperature", "max_tokens", "timeout", "max_results", "search_depth"]:
                    env_var_name = f"{node_type.upper()}_{key.upper()}"
                    if env_var_name not in used_env_vars:
                        node_config[env_var_name] = str(value)
                        used_env_vars.add(env_var_name)
            
            # Add credentials (prioritize user input)
            for key, value in node_data.items():
                if any(cred_key in key.lower() for cred_key in ['api_key', 'token', 'secret', 'password', 'key']):
                    env_var_name = f"{node_type.upper()}_{key.upper()}"
                    if env_var_name not in used_env_vars:
                        user_value = user_env_vars.get(env_var_name, str(value))
                        node_config[env_var_name] = str(user_value)
                        used_env_vars.add(env_var_name)
            
            if node_config:
                node_sections[node_type].append((node_id, node_config))
    
    # Write node sections
    if node_sections:
        env_content.append("# Node Configuration Variables")
        for node_type, nodes_list in node_sections.items():
            env_content.append(f"# {node_type} Nodes Configuration")
            
            # Combine all configs for this node type
            combined_config = {}
            for node_id, config in nodes_list:
                combined_config.update(config)
            
            # Write unique variables for this node type
            for env_var, value in combined_config.items():
                env_content.append(f"{env_var}={value}")
            env_content.append("")
    
    return "\n".join(env_content)

def create_ready_to_run_docker_context(
    workflow: Workflow,
    minimal_backend: Dict[str, str], 
    pre_configured_env: str,
    docker_config: DockerConfig
) -> Dict[str, str]:
    """Create ready-to-run Docker context files."""
    logger.info("Creating Docker context")
    
    # docker-compose.yml
    docker_compose = f'''services:
  # Workflow API Backend (RESTful API only)
  workflow-api:
    build: .
    env_file:
      - .env
    environment:
      # Override critical settings to ensure they're set
      - WORKFLOW_MODE=runtime
    ports:
      - "{docker_config.docker_port}:{docker_config.api_port}"
    volumes:
      - ./logs:/app/logs
      - workflow_memory:/app/memory
    healthcheck:
      test: ["CMD", "sh", "-c", "curl -f http://localhost:$$API_PORT/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    networks:
      - workflow-runtime-network

volumes:
  logs:
  workflow_memory:
    driver: local

networks:
  workflow-runtime-network:
    driver: bridge
'''
    
    return {
        "docker-compose.yml": docker_compose
    }

def generate_ready_to_run_readme(workflow_name: str, env_config: WorkflowEnvironmentConfig) -> str:
    """Generate README for exported workflow."""
    
    # Get formatted date outside f-string to avoid backslash issue
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    workflow_slug = workflow_name.lower().replace(' ', '-')
    
    # Build API key section
    api_key_status = "WITH API KEY:" if env_config.security.require_api_key else "WITHOUT API KEY:"
    api_key_header = '  -H "X-API-Key: YOUR_API_KEY" \\\n  # OR -H "Authorization: Bearer YOUR_API_KEY" \\' if env_config.security.require_api_key else ''
    
    # Build security status
    auth_status = "✅ API Key authentication is ENABLED" if env_config.security.require_api_key else "⚠️ API Key authentication is DISABLED"
    host_status = "✅ All hosts allowed for maximum accessibility" if env_config.security.allowed_hosts == "*" else f"⚠️ Limited to hosts: {env_config.security.allowed_hosts}"
    
    # Build monitoring status  
    monitoring_status = "✅ LangSmith monitoring is ENABLED" if env_config.monitoring.enable_langsmith else "❌ LangSmith monitoring is DISABLED"
    
    readme = f"""# {workflow_name} - Docker Export

This is a ready-to-run Docker export of your KAI-Fusion workflow.

## Quick Start

1. **Extract the package:**
   ```bash
   unzip workflow-export-{workflow_slug}.zip
   cd workflow-export-{workflow_slug}/
   ```

2. **Start the workflow:**
   ```bash
   docker-compose up -d
   ```

3. **Check status:**
   ```bash
   docker-compose ps
   docker-compose logs workflow-api
   ```

4. **Test the API:**
   ```bash
   curl http://localhost:{env_config.docker.docker_port}/health
   curl http://localhost:{env_config.docker.docker_port}/api/workflow/info
   ```

## API Usage

### Execute Workflow
{api_key_status}
```bash
curl -X POST http://localhost:{env_config.docker.docker_port}/api/workflow/execute \\
  -H "Content-Type: application/json" \\
{api_key_header}
  -d '{{"input": "Your input here", "parameters": {{}}}}'
```

### Available Endpoints
- `GET /health` - Health check
- `GET /api/workflow/info` - Workflow information
- `POST /api/workflow/execute` - Execute workflow
- `GET /api/workflow/status/{{execution_id}}` - Check execution status
- `GET /api/workflow/result/{{execution_id}}` - Get execution result

## Configuration

### Environment Variables
The `.env` file contains all pre-configured settings. You can modify:

- `DOCKER_PORT` - Change the external port
- `API_KEYS` - Update API keys for access control
- `REQUIRE_API_KEY` - Enable/disable API key authentication

### Security
{auth_status}
{host_status}

### Monitoring
{monitoring_status}

 ## Troubleshooting
 
 ### Check Container Status
 ```bash
 docker-compose ps
 docker-compose logs -f workflow-api
 ```
 
 ### Port Connection Issues
 If you can't access http://localhost:{env_config.docker.docker_port}:
 
 1. **Check if container is running:**
    ```bash
    docker-compose ps
    ```
 
 2. **Check port mapping:**
    ```bash
    docker port $(docker-compose ps -q workflow-api)
    ```
 
 3. **Test internal connection:**
    ```bash
    docker-compose exec workflow-api sh -c 'curl http://localhost:$API_PORT/health'
    ```
 
 4. **Windows Docker Desktop - try 127.0.0.1:**
    ```bash
    curl http://127.0.0.1:{env_config.docker.docker_port}/health
    ```
 
 ### Restart Services
 ```bash
 docker-compose restart
 docker-compose down && docker-compose up -d
 ```
 
 ### Rebuild if Needed
 ```bash
 docker-compose down
 docker-compose build --no-cache
 docker-compose up -d
 ```

## Support

This workflow was exported from KAI-Fusion. For support, please refer to your KAI-Fusion documentation.

Generated on: {current_time}
"""
    
    return readme

def create_workflow_export_package(components: Dict[str, Any]) -> Dict[str, Any]:
    """Create the final export package as a ZIP file."""
    logger.info("Creating workflow export package")
    
    workflow = components['workflow']
    
    # Create temporary directory
    with tempfile.TemporaryDirectory() as temp_dir:
        package_name = f"workflow-export-{workflow.name.lower().replace(' ', '-')}"
        package_dir = os.path.join(temp_dir, package_name)
        os.makedirs(package_dir)
        
        # Create organized directory structure
        app_dir = os.path.join(package_dir, "app")
        api_dir = os.path.join(app_dir, "api")
        models_dir = os.path.join(app_dir, "models")
        core_dir = os.path.join(app_dir, "core")
        
        os.makedirs(app_dir)
        os.makedirs(api_dir)
        os.makedirs(models_dir)
        os.makedirs(core_dir)
        os.makedirs(os.path.join(package_dir, "logs"))
        
        # Write root level configuration files
        with open(os.path.join(package_dir, ".env"), 'w', encoding='utf-8') as f:
            f.write(components['pre_configured_env'])
        
        with open(os.path.join(package_dir, "Dockerfile"), 'w', encoding='utf-8') as f:
            f.write(components['backend']['Dockerfile'])
        
        with open(os.path.join(package_dir, "docker-compose.yml"), 'w', encoding='utf-8') as f:
            f.write(components['docker_configs']['docker-compose.yml'])
        
        with open(os.path.join(package_dir, "requirements.txt"), 'w', encoding='utf-8') as f:
            f.write(components['filtered_requirements'])
        
        with open(os.path.join(package_dir, "README.md"), 'w', encoding='utf-8') as f:
            f.write(components['readme'])
        
        # Write application structure
        with open(os.path.join(package_dir, "main.py"), 'w', encoding='utf-8') as f:
            f.write(components['backend']['main.py'])
        
        # Write API files
        with open(os.path.join(api_dir, "__init__.py"), 'w', encoding='utf-8') as f:
            f.write('"""API module for workflow execution."""\n')
        
        with open(os.path.join(api_dir, "workflow.py"), 'w', encoding='utf-8') as f:
            f.write(components['backend'].get('api_workflow', generate_workflow_api_code()))
        
        with open(os.path.join(api_dir, "health.py"), 'w', encoding='utf-8') as f:
            f.write(components['backend'].get('api_health', generate_health_api_code()))
        
        # Write Models files
        with open(os.path.join(models_dir, "__init__.py"), 'w', encoding='utf-8') as f:
            f.write('"""Data models for workflow execution."""\n')
        
        with open(os.path.join(models_dir, "workflow.py"), 'w', encoding='utf-8') as f:
            f.write(components['backend'].get('models_workflow', generate_workflow_models_code()))
        
        with open(os.path.join(models_dir, "execution.py"), 'w', encoding='utf-8') as f:
            f.write(components['backend'].get('models_execution', generate_execution_models_code()))
        
        # Write Core files
        with open(os.path.join(core_dir, "__init__.py"), 'w', encoding='utf-8') as f:
            f.write('"""Core utilities and configuration."""\n')
        
        with open(os.path.join(core_dir, "config.py"), 'w', encoding='utf-8') as f:
            f.write(components['backend'].get('core_config', generate_core_config_code()))
        
        with open(os.path.join(core_dir, "database.py"), 'w', encoding='utf-8') as f:
            f.write(components['backend'].get('core_database', generate_core_database_code()))
        
        # Write main app init
        with open(os.path.join(app_dir, "__init__.py"), 'w', encoding='utf-8') as f:
            f.write('"""Workflow execution application."""\n')
        
        # Write workflow definition
        with open(os.path.join(package_dir, "workflow-definition.json"), 'w', encoding='utf-8') as f:
            json.dump(workflow.flow_data, f, indent=2, ensure_ascii=False)
        
        # Create ZIP file
        zip_path = f"{package_dir}.zip"
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for root, dirs, files in os.walk(package_dir):
                for file in files:
                    file_path = os.path.join(root, file)
                    arcname = os.path.relpath(file_path, temp_dir)
                    zipf.write(file_path, arcname)
        
        # Calculate size
        package_size = os.path.getsize(zip_path)
        
        # Move to permanent storage
        import shutil
        permanent_dir = os.path.join(os.getcwd(), "exports")
        os.makedirs(permanent_dir, exist_ok=True)
        
        permanent_zip_path = os.path.join(permanent_dir, f"{package_name}.zip")
        shutil.move(zip_path, permanent_zip_path)
        
        download_url = f"/api/v1/export/download/{package_name}.zip"
        
        return {
            "download_url": download_url,
            "package_size": package_size,
            "local_path": permanent_zip_path
        }

@router.get("/export/download/{filename}", tags=["Export"])
async def download_export_package(filename: str):
    """Download exported workflow package."""
    try:
        # Validate filename to prevent directory traversal
        if ".." in filename or "/" in filename or "\\" in filename:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Invalid filename"
            )
        
        # Construct file path
        exports_dir = os.path.join(os.getcwd(), "exports")
        file_path = os.path.join(exports_dir, filename)
        
        # Check if file exists
        if not os.path.exists(file_path):
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Export package not found"
            )
        
        # Return file response
        return FileResponse(
            path=file_path,
            filename=filename,
            media_type="application/zip"
        )
        
    except Exception as e:
        logger.error(f"Download failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Download failed"
        )

@router.get("/export/workflows", tags=["Export"])
async def export_workflows():
    """Export workflows data."""
    try:
        # Placeholder for future export functionality
        return {
            "status": "success",
            "message": "Export functionality coming soon",
            "data": {}
        }
    except Exception as e:
        logger.error(f"Export failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Export operation failed"
        )

@router.get("/export/nodes", tags=["Export"])
async def export_nodes():
    """Export nodes configuration."""
    try:
        # Placeholder for future export functionality
        return {
            "status": "success", 
            "message": "Node export functionality coming soon",
            "data": {}
        }
    except Exception as e:
        logger.error(f"Node export failed: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Node export operation failed"
        )


def generate_workflow_api_code() -> str:
    """Generate workflow API endpoints code."""
    return '''# -*- coding: utf-8 -*-
"""Workflow execution API endpoints."""

from fastapi import APIRouter, HTTPException, Depends
from typing import Dict, Any, Optional
from pydantic import BaseModel
import logging
import uuid
import json
import os
from datetime import datetime

from app.models.workflow import WorkflowExecution, WorkflowExecutionRequest
from app.core.database import get_db_session

logger = logging.getLogger(__name__)
router = APIRouter()

class ExecuteWorkflowRequest(BaseModel):
    """Request model for workflow execution."""
    input_data: Optional[Dict[str, Any]] = None
    execution_config: Optional[Dict[str, Any]] = None

class ExecuteWorkflowResponse(BaseModel):
    """Response model for workflow execution."""
    execution_id: str
    status: str
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

# Load workflow definition
def load_workflow_definition():
    try:
        with open('workflow-definition.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"Failed to load workflow definition: {e}")
        return {"nodes": [], "edges": []}

WORKFLOW_DEF = load_workflow_definition()

def execute_llm_workflow(user_input: str, session_id: str = None, parameters: Dict[str, Any] = None) -> Dict[str, Any]:
    """Execute LLM workflow with user input and session memory."""
    try:
        # Find LLM and Memory nodes in workflow
        llm_nodes = [node for node in WORKFLOW_DEF.get("nodes", []) if "openai" in node.get("type", "").lower() or "chat" in node.get("type", "").lower()]
        memory_nodes = [node for node in WORKFLOW_DEF.get("nodes", []) if "memory" in node.get("type", "").lower()]
        
        if not llm_nodes:
            return {"response": "No LLM nodes found in workflow", "type": "error"}
        
        # Initialize memory system
        session_memory = []
        if session_id and memory_nodes:
            session_memory = load_session_memory(session_id)
        
        # Simple LLM integration - use OpenAI if available
        try:
            import openai
            
            # Get API key from environment
            api_key = os.getenv("OPENAI_API_KEY") or os.getenv("OPENAICHAT_API_KEY")
            if not api_key:
                return {"response": "OpenAI API key not configured", "type": "error"}
            
            # Initialize OpenAI client
            client = openai.OpenAI(api_key=api_key)
            
            # Get model from workflow or use default
            model = "gpt-3.5-turbo"
            temperature = 0.7
            max_tokens = 2048
            
            # Extract model config from first LLM node
            if llm_nodes:
                node_data = llm_nodes[0].get("data", {})
                model = node_data.get("model", model)
                temperature = float(node_data.get("temperature", temperature))
                max_tokens = int(node_data.get("max_tokens", max_tokens))
            
            # Build messages with memory
            messages = [{"role": "system", "content": "You are a helpful AI assistant integrated into KAI-Fusion workflow system."}]
            
            # Add memory context if available
            if session_memory:
                # Add recent conversation history (last 10 messages)
                recent_memory = session_memory[-10:] if len(session_memory) > 10 else session_memory
                for mem in recent_memory:
                    if mem.get("role") in ["user", "assistant"]:
                        messages.append({"role": mem["role"], "content": mem["content"]})
            
            # Add current user input
            messages.append({"role": "user", "content": user_input})
            
            # Create chat completion
            response = client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            assistant_response = response.choices[0].message.content
            
            # Save to memory if memory nodes exist and session_id provided
            if session_id and memory_nodes:
                save_to_session_memory(session_id, user_input, assistant_response)
            
            return {
                "response": assistant_response,
                "type": "success",
                "model": model,
                "session_id": session_id,
                "memory_enabled": bool(memory_nodes and session_id),
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                }
            }
            
        except ImportError:
            return {"response": "OpenAI library not installed", "type": "error"}
        except Exception as e:
            logger.error(f"OpenAI API error: {e}")
            return {"response": f"LLM execution failed: {str(e)}", "type": "error"}
    
    except Exception as e:
        logger.error(f"Workflow execution error: {e}")
        return {"response": f"Workflow execution failed: {str(e)}", "type": "error"}

def load_session_memory(session_id: str) -> list:
    """Load conversation memory for a session."""
    try:
        memory_file = f"memory/session_{session_id}.json"
        if os.path.exists(memory_file):
            with open(memory_file, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        logger.error(f"Failed to load session memory: {e}")
    return []

def save_to_session_memory(session_id: str, user_input: str, assistant_response: str):
    """Save conversation to session memory."""
    try:
        # Create memory directory if it doesn't exist
        os.makedirs("memory", exist_ok=True)
        
        memory_file = f"memory/session_{session_id}.json"
        
        # Load existing memory
        memory = load_session_memory(session_id)
        
        # Add new conversation
        timestamp = datetime.now().isoformat()
        memory.extend([
            {"role": "user", "content": user_input, "timestamp": timestamp},
            {"role": "assistant", "content": assistant_response, "timestamp": timestamp}
        ])
        
        # Keep only last 100 messages to prevent memory file from growing too large
        if len(memory) > 100:
            memory = memory[-100:]
        
        # Save updated memory
        with open(memory_file, 'w', encoding='utf-8') as f:
            json.dump(memory, f, indent=2, ensure_ascii=False)
            
    except Exception as e:
        logger.error(f"Failed to save session memory: {e}")

@router.post("/execute", response_model=ExecuteWorkflowResponse)
async def execute_workflow(
    request: ExecuteWorkflowRequest,
    db = Depends(get_db_session)
):
    """Execute the workflow with provided input data."""
    try:
        execution_id = str(uuid.uuid4())
        
        # Get user input from request
        user_input = ""
        session_id = None
        
        if hasattr(request, 'input_data') and request.input_data:
            user_input = request.input_data.get("input", "") or request.input_data.get("message", "")
            session_id = request.input_data.get("session_id")
        
        # If no session_id provided, generate one for memory tracking
        if not session_id:
            session_id = f"session_{str(uuid.uuid4())[:8]}"
        
        if not user_input:
            return ExecuteWorkflowResponse(
                execution_id=execution_id,
                status="failed",
                error="No input provided",
                result=None
            )
        
        logger.info(f"Executing workflow with input: {user_input}, session: {session_id}")
        
        # Execute LLM workflow with session memory
        result = execute_llm_workflow(user_input, session_id, request.execution_config or {})
        
        return ExecuteWorkflowResponse(
            execution_id=execution_id,
            status="completed" if result.get("type") == "success" else "failed",
            result=result,
            error=None if result.get("type") == "success" else result.get("response")
        )
        
    except Exception as e:
        logger.error(f"Workflow execution failed: {e}")
        return ExecuteWorkflowResponse(
            execution_id=str(uuid.uuid4()),
            status="failed",
            error=str(e),
            result=None
        )

@router.get("/executions/{execution_id}")
async def get_execution_status(execution_id: str):
    """Get execution status and results."""
    try:
        return {
            "execution_id": execution_id,
            "status": "completed",
            "result": {"message": "Execution completed"},
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Failed to get execution status: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/info")
async def get_workflow_info():
    """Get workflow information."""
    try:
        llm_nodes = [node for node in WORKFLOW_DEF.get("nodes", []) if "openai" in node.get("type", "").lower() or "chat" in node.get("type", "").lower()]
        memory_nodes = [node for node in WORKFLOW_DEF.get("nodes", []) if "memory" in node.get("type", "").lower()]
        
        return {
            "workflow": WORKFLOW_DEF,
            "nodes_count": len(WORKFLOW_DEF.get("nodes", [])),
            "edges_count": len(WORKFLOW_DEF.get("edges", [])),
            "llm_nodes": llm_nodes,
            "memory_nodes": memory_nodes,
            "memory_enabled": len(memory_nodes) > 0,
            "status": "ready"
        }
    except Exception as e:
        logger.error(f"Failed to get workflow info: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/memory/{session_id}")
async def get_session_memory(session_id: str):
    """Get conversation memory for a session."""
    try:
        memory = load_session_memory(session_id)
        return {
            "session_id": session_id,
            "messages": memory,
            "message_count": len(memory),
            "memory_enabled": True
        }
    except Exception as e:
        logger.error(f"Failed to get session memory: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.delete("/memory/{session_id}")
async def clear_session_memory(session_id: str):
    """Clear conversation memory for a session."""
    try:
        memory_file = f"memory/session_{session_id}.json"
        if os.path.exists(memory_file):
            os.remove(memory_file)
        
        return {
            "session_id": session_id,
            "status": "cleared",
            "message": "Session memory cleared successfully"
        }
    except Exception as e:
        logger.error(f"Failed to clear session memory: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/sessions")
async def list_active_sessions():
    """List all active sessions with memory."""
    try:
        sessions = []
        memory_dir = "memory"
        
        if os.path.exists(memory_dir):
            for filename in os.listdir(memory_dir):
                if filename.startswith("session_") and filename.endswith(".json"):
                    session_id = filename.replace("session_", "").replace(".json", "")
                    memory = load_session_memory(session_id)
                    
                    # Get last message timestamp
                    last_timestamp = None
                    if memory:
                        last_message = memory[-1]
                        last_timestamp = last_message.get("timestamp")
                    
                    sessions.append({
                        "session_id": session_id,
                        "message_count": len(memory),
                        "last_activity": last_timestamp
                    })
        
        return {
            "sessions": sessions,
            "total_sessions": len(sessions)
        }
    except Exception as e:
        logger.error(f"Failed to list sessions: {e}")
        raise HTTPException(status_code=500, detail=str(e))
'''


def generate_health_api_code() -> str:
    """Generate health check API code."""
    return '''# -*- coding: utf-8 -*-
"""Health check API endpoints."""

from fastapi import APIRouter
from typing import Dict, Any
import logging
import os

logger = logging.getLogger(__name__)
router = APIRouter()

@router.get("/health")
async def health_check() -> Dict[str, Any]:
    """Health check endpoint."""
    return {
        "status": "healthy",
        "service": "workflow-runtime",
        "version": "1.0.0",
        "environment": os.getenv("ENVIRONMENT", "production")
    }

@router.get("/status")
async def status_check() -> Dict[str, Any]:
    """Detailed status check."""
    return {
        "status": "healthy",
        "checks": {
            "database": "connected",
            "workflow_engine": "ready",
            "api": "operational"
        },
        "uptime": "running",
        "configuration": {
            "workflow_mode": os.getenv("WORKFLOW_MODE", "runtime"),
            "api_port": os.getenv("API_PORT", "8000")
        }
    }
'''


def generate_workflow_models_code() -> str:
    """Generate workflow data models code."""
    return '''# -*- coding: utf-8 -*-
"""Workflow data models."""

from sqlalchemy import Column, String, Text, DateTime, JSON
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
from pydantic import BaseModel
from typing import Dict, Any, Optional
from datetime import datetime
import uuid

Base = declarative_base()

class WorkflowExecution(Base):
    """Workflow execution database model."""
    __tablename__ = "workflow_executions"
    
    execution_id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    status = Column(String, nullable=False, default="pending")
    input_data = Column(JSON, nullable=True)
    output_data = Column(JSON, nullable=True)
    execution_config = Column(JSON, nullable=True)
    error_message = Column(Text, nullable=True)
    created_at = Column(DateTime, default=func.now())
    updated_at = Column(DateTime, default=func.now(), onupdate=func.now())

class WorkflowExecutionRequest(BaseModel):
    """Pydantic model for workflow execution requests."""
    input_data: Optional[Dict[str, Any]] = None
    execution_config: Optional[Dict[str, Any]] = None

class WorkflowExecutionResponse(BaseModel):
    """Pydantic model for workflow execution responses."""
    execution_id: str
    status: str
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    created_at: datetime
    updated_at: datetime
'''


def generate_execution_models_code() -> str:
    """Generate execution-specific models code."""
    return '''# -*- coding: utf-8 -*-
"""Execution tracking models."""

from sqlalchemy import Column, String, Text, DateTime, Integer, Float
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
from pydantic import BaseModel
from typing import Optional
from datetime import datetime
import uuid

Base = declarative_base()

class ExecutionLog(Base):
    """Execution log database model."""
    __tablename__ = "execution_logs"
    
    log_id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    execution_id = Column(String, nullable=False)
    log_level = Column(String, nullable=False, default="INFO")
    message = Column(Text, nullable=False)
    timestamp = Column(DateTime, default=func.now())
    node_id = Column(String, nullable=True)
    step_number = Column(Integer, nullable=True)

class ExecutionMetrics(Base):
    """Execution metrics database model."""
    __tablename__ = "execution_metrics"
    
    metric_id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    execution_id = Column(String, nullable=False)
    metric_name = Column(String, nullable=False)
    metric_value = Column(Float, nullable=False)
    timestamp = Column(DateTime, default=func.now())

class ExecutionLogEntry(BaseModel):
    """Pydantic model for execution log entries."""
    log_level: str
    message: str
    node_id: Optional[str] = None
    step_number: Optional[int] = None
    timestamp: datetime

class ExecutionMetric(BaseModel):
    """Pydantic model for execution metrics."""
    metric_name: str
    metric_value: float
    timestamp: datetime
'''


def generate_core_config_code() -> str:
    """Generate core configuration code."""
    return '''# -*- coding: utf-8 -*-
"""Core configuration settings."""

import os
from typing import Optional

class Settings:
    """Application settings."""
    
    # API Configuration - Use dynamic port from environment
    API_PORT: int = int(os.getenv("API_PORT", "8000"))
    HOST: str = os.getenv("API_HOST", "0.0.0.0")
    
    # Database Configuration
    DATABASE_URL: str = os.getenv("DATABASE_URL", "sqlite:///./workflow.db")
    
    # Workflow Configuration
    WORKFLOW_ID: str = os.getenv("WORKFLOW_ID", "unknown")
    WORKFLOW_MODE: str = os.getenv("WORKFLOW_MODE", "runtime")
    
    # Security Configuration
    API_KEYS: Optional[str] = os.getenv("API_KEYS", None)
    REQUIRE_API_KEY: str = os.getenv("REQUIRE_API_KEY", "false")
    
    # Monitoring Configuration
    ENABLE_LANGSMITH: bool = os.getenv("ENABLE_LANGSMITH", "false").lower() == "true"
    LANGCHAIN_API_KEY: Optional[str] = os.getenv("LANGCHAIN_API_KEY", None)
    LANGSMITH_PROJECT: Optional[str] = os.getenv("LANGSMITH_PROJECT", None)
    
    # Logging Configuration
    LOG_LEVEL: str = os.getenv("LOG_LEVEL", "INFO")
    LOG_FORMAT: str = os.getenv("LOG_FORMAT", "%(asctime)s - %(name)s - %(levelname)s - %(message)s")

# Global settings instance
settings = Settings()
'''


def generate_core_database_code() -> str:
    """Generate core database utilities code."""
    return '''# -*- coding: utf-8 -*-
"""Database configuration and utilities."""

from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy.ext.declarative import declarative_base
import logging

from app.core.config import settings

logger = logging.getLogger(__name__)

# Create async engine with asyncpg configuration
engine = create_async_engine(
    settings.DATABASE_URL.replace("postgresql://", "postgresql+asyncpg://") 
    if settings.DATABASE_URL.startswith("postgresql://") 
    else settings.DATABASE_URL,
    echo=False,
    # Disable prepared statement cache to avoid conflicts
    connect_args={"statement_cache_size": 0} if "postgresql" in settings.DATABASE_URL else {}
)

# Create async session factory
AsyncSessionLocal = sessionmaker(
    engine, class_=AsyncSession, expire_on_commit=False
)

Base = declarative_base()

async def get_db_session() -> AsyncSession:
    """Get database session."""
    async with AsyncSessionLocal() as session:
        try:
            yield session
        finally:
            await session.close()

async def init_database():
    """Initialize database tables."""
    try:
        async with engine.begin() as conn:
            await conn.run_sync(Base.metadata.create_all)
        logger.info("Database initialized successfully")
    except Exception as e:
        logger.error(f"Database initialization failed: {e}")
        raise

async def close_database():
    """Close database connections."""
    try:
        await engine.dispose()
        logger.info("Database connections closed")
    except Exception as e:
        logger.error(f"Error closing database: {e}")
'''