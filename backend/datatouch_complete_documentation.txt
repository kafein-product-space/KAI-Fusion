DATATOUCH DOCUMENTATION - COMPLETE EXTRACTION
================================================================================
Extracted from 46 pages
Total content length: 102,266 characters
Extraction date: 2025-08-04 22:26:21.141949
================================================================================


================================================================================
SECTION 1: https://datatouch.netfein.com/latest/
Content Length: 307 characters
================================================================================

Data Touch Skip to content Welcome to Data Touch ¶ Data Touch DAM is a comprehensive tool that retrieves data from Dataskope DAM visualizes it through heatmaps and data lineage concepts connects to various databases to generate reports and performs risk assessments on a user and database basis. Back to top


================================================================================
SECTION 2: https://datatouch.netfein.com/latest/install/poc/
Content Length: 2,423 characters
================================================================================

PoC - Data Touch Skip to content PoC Installation Requirements ¶ This document provides a detailed overview of the requirements the Proof of Concept PoC installation phase of the Data Touch product developed by Kafein Technology Solutions . Meeting the requirements outlined below is crucial to ensuring the smooth and efficient operation of Data Touch. 1. System Requirements ¶ Environment Server Type Server Name OS CPU RAM Disk PoC Application DT-Master Ubuntu 8 16GB 100GB PoC Anomaly DT-Analyst Ubuntu 8 16GB 100GB PoC Database DT-Sage Ubuntu 8 16GB 100GB 2. Network Requirements ¶ The following ports must be open during the installation phase: 22 80 443 5432 8080 3. Internal Requirements ¶ There should be no network restrictions between the three servers to ensure proper communication between the applications. 4. External Requirements ¶ Since software like Docker OpenSSL and Nginx needs to be installed during setup the provided servers must have access to the external network . If internet access is not available the required software packages will be provided and must be manually placed on the servers. 5. Operating System Requirements ¶ Data Touch operates on the Ubuntu operating system a Debian-based Linux distribution. Supported OS versions: Ubuntu 20.04 Ubuntu 22.04 Windows Server 2019 or later 6. DNS Requirements Optional ¶ To simplify the installation process using DNS is recommended. Example DNS configuration: datatouch.poc.kafein.com 7. SSL Requirements Optional ¶ In installations where DNS is used either self-signed or CA-signed certificates may be used: DNS redirection will be handled by the team providing the environment . A self-signed certificate can be created during installation. A CA-signed certificate must be provided by the customer. The DNS and certificate details should be finalized before installation . 8. Test and Utility Tools ¶ Postman Collection The Postman collection has been prepared to facilitate the testing validation and debugging of API requests within the Data Touch project. It includes a variety of endpoints and sample request structures tailored to different scenarios. This collection can be used to verify the correct operation of API integrations and identify potential performance issues. It provides detailed examples key operations such as token retrieval data filtering user queries and data usage analysis. Download Postman Collection Back to top


================================================================================
SECTION 3: https://datatouch.netfein.com/latest/install/beforeinstall/
Content Length: 2,959 characters
================================================================================

Before Installation - Data Touch Skip to content Before Installation The installation of Data Touch consists of 11 key steps. For a successful installation each step must be completed in the correct order. Note Ensure the Dataskope DAM is installed and configured before proceeding with the installation of Data Touch. Data Touch needs to be able to communicate with the DAM and Elasticsearch to properly. Warning If you re installing on Windows Server please update PowerShell to version 6.0 or newer. To update you can use the MSI package in the Scripts directory or visit: https://aka.ms/PSWindows The steps are outlined below. 1. Docker Installation 2. Extracting Packages 3. Loading Docker Images 4. Init Configuration 5. Hostname Configuration 6. Dataskope Configuration 7. Metadata Configuration 8. HTTPS Configuration Optional 9. Deployment 10. After Deployment 11. Monitoring Supported Browser Versions ¶ Google Chrome Latest Version: 116.0.5845.96 Previous Versions: 115.0.5790.98 114.0.5735.198 Mozilla Firefox Latest Version: 116.0 Previous Versions: 115.0 114.0 Microsoft Edge Latest Version: 116.0.1938.62 Previous Versions: 115.0.1901.188 114.0.1823.67 Safari macOS Latest Version: 16.6 Previous Versions: 16.5 16.4 System Requirements ¶ For optimal performance the hardware requirements vary based on the deployment type PoC or Production and the scale of your database table and event counts. Below are the minimum system requirements: Proof of Concept PoC ¶ Operating System : Linux-based distributions Ubuntu CentOS etc. are recommended and Windows Server 2022 or newer is also supported. Processor : 8 CPU cores Memory RAM : 16 GB RAM Storage : 500 GB disk space Production ¶ Operating System : Linux-based distributions Ubuntu CentOS etc. are recommended and Windows Server 2022 or newer is also supported. Processor : 16 CPU cores Memory RAM : 32 GB RAM Storage : 1 TB disk space Note The actual hardware requirements may vary depending on the size of your database the number of tables and the volume of events being processed. Network Requirements ¶ Data Touch is a web application that operates both at the backend and frontend. We utilize Nginx as a reverse proxy to manage traffic to our applications including those running in Docker containers. Ports Required ¶ HTTP : 80 HTTPS : 443 Dataskope ¶ Data Touch needs to reach the Dataskope DAM authentication and fetching data. The following ports are required Dataskope DAM to properly: 8443 9200 Data Focus ¶ Data Touch needs to reach Data Focus as Metadata Service. The following ports are required Data Focus to properly: 80 443 SDM ¶ Data Touch needs to reach SDM as Metadata Service. The following ports are required SDM to properly: 8080 8443 This ports are optional can be change on installation phase. Please confirm before the start and ensure that these ports are open and properly configured to allow seamless communication between clients and the application. Back to top


================================================================================
SECTION 4: https://datatouch.netfein.com/latest/install/1docker/
Content Length: 3,676 characters
================================================================================

Docker Installation - Data Touch Skip to content Docker Installation Guide ¶ This page provides instructions installing Docker on both Linux and Windows systems. Always refer to official documentation the most accurate and up-to-date instructions. Offline installers are available upon request. Offline docker installation is supported. Download from Product Releases . For any problem don t hesitate contact with email protected Docker Installation Linux ¶ mkdocs serve System Requirements ¶ Supported Distributions : Docker Engine supports a variety of Linux distributions including Ubuntu Debian CentOS Fedora and others. Docker Version : Docker Engine 26 or higher is recommended optimal compatibility. Steps to Install Docker on Linux ¶ Refer to the Official Documentation: Visit the Official Docker Installation Guide detailed distribution-specific instructions. Enable and Start Docker : Enable Docker to start on boot and start the service: Bash sudo systemctl enable docker sudo systemctl start docker Verify Installation : Verify that Docker is installed and running: Bash docker --version docker run hello-world Docker Installation Windows ¶ For Windows WSL 2 is a mandatory backend running Docker. System Requirements ¶ WSL Version : 1.1.3.0 or later. Operating System : Windows Server 2022 build 20348 or higher is recommended. Processor : A 64-bit processor with Second Level Address Translation SLAT support. Hardware Virtualization : Ensure Nested Virtualization is enabled in the BIOS settings. For more details refer to the Virtualization Overview . Steps to Install Docker on Windows ¶ 1. Enable WSL 2 on Windows ¶ Follow the official Microsoft WSL installation guide: Install WSL . Open PowerShell as Administrator and run the following commands to enable WSL and set WSL 2 as the default version: PowerShell dism . exe / online / enable-feature / featurename : Microsoft-Windows-Subsystem-Linux / all / norestart dism . exe / online / enable-feature / featurename : VirtualMachinePlatform / all / norestart wsl - -set-default-version 2 Restart your computer after enabling these features. 2. Install Docker Desktop ¶ Download and install Docker Desktop from the Product Releases . During installation select the Use WSL 2 instead of Hyper-V option. 3. Verify Docker Installation ¶ Open a terminal PowerShell Command Prompt or WSL and run: Bash docker version docker run hello-world Offline Installation ¶ For environments without internet access we can provide offline installers Docker. You can find at Offline Docker Releases . Please contact email protected to request offline installation packages and instructions. Verify Docker Installation ¶ After installing Docker ensure that it is correctly set up by performing the following steps: Check Docker Version : Run the following command to verify the installed Docker version: Bash docker version Run a Test Container : Confirm Docker functionality by running the hello-world container: Bash docker run hello-world If the installation is successful you should see a message indicating that Docker is working correctly. Hello from Docker! This message shows that your installation appears to be working correctly. Check Resource Allocation : For performance issues ensure that sufficient resources are allocated in Docker Desktop settings Windows or system configurations Linux . Important Notes ¶ Always refer to the official Docker documentation the latest installation instructions: Docker Installation Guide Docker Desktop Windows Hardware Prerequisites WSL 2 : A 64-bit processor with Second Level Address Translation SLAT support. Hardware virtualization must be enabled in the BIOS. Back to top


================================================================================
SECTION 5: https://datatouch.netfein.com/latest/install/2extractpackages/
Content Length: 551 characters
================================================================================

Extracting Packages - Data Touch Extracting Packages You will receive a .tar.gz package such as datatouch-3.3.1.tar.gz . Follow these steps to extract it: Linux Windows Bash mkdir datatouch tar -xvzf datatouch-3.3.1.tar.gz -C path/to/your/datatouch PowerShell New-Item -ItemType Directory -Name datatouch tar -xvzf datatouch - 3 . 3 . 1 . tar . gz -C path / to / your / datatouch If you are using Windows you can also create folder via UI and extract the package via Explorer. Windows Server 2022 and higher releases includes tar built-in. Back to top


================================================================================
SECTION 6: https://datatouch.netfein.com/latest/install/3loadimages/
Content Length: 721 characters
================================================================================

Loading Docker Images - Data Touch Loading Docker Images After extracting the tar.gz package the Docker image needs to be loaded into your system. These compressed Docker images are created using the docker save command with compression reducing the package size by over 70%. Estimated Time: The loading process typically takes 3â4 minutes depending on your CPU performance. If the process takes longer than 5 minutes it may indicate that your CPU does not meet the minimum performance requirements. To load the Docker images: Linux Windows Bash ./config.sh load_images PowerShell ./ config . ps1 load_images To verify the imported images using: Linux Windows Bash docker image ls PowerShell docker image ls Back to top


================================================================================
SECTION 7: https://datatouch.netfein.com/latest/install/4initconf/
Content Length: 957 characters
================================================================================

Init Configuration - Data Touch Init Configuration The Data Touch application can use SDM or Data Focus as its Metadata service. Before deploying Data Touch you need to configure some parameters to ensure it works correctly. Be sure to confirm the deployment type before proceeding with the configuration. Please check the config.sh file exists in the same folder as the docker-compose file. This script will help you configure the Data Touch application. Avoid using root user of the system. You must use the different user with sudo rights. The config.sh script creates a backup of the docker-compose file with each run. You can find previous versions of the compose file in the backup folder and restore them needed. The script automatically configures the compose file but you need advanced configurations you can manually update the variables without using config.sh . You can use editors like Vim or Nano to modify the docker-compose file. Back to top


================================================================================
SECTION 8: https://datatouch.netfein.com/latest/install/5hostname/
Content Length: 1,780 characters
================================================================================

Hostname Configuration - Data Touch Skip to content Hostname Configuration Hostname Configuration with set_hostname Script ¶ The set_hostname script is a utility provided with the application to help configure the necessary hostname parameters. This script ensures that the correct browser URL is set up the application including: Frontend Configuration : Ensures the browser can access the application. Backend Configuration : Allows proper routing of requests to the backend services. CORS Cross-Origin Resource Sharing : Configures the application to handle requests from different origins securely. Setting the hostname correctly is essential users to access the application through their web browsers and the application to handle requests and responses efficiently. Purpose of the set_hostname Script ¶ The set_hostname script specifically configures the browser URL the application. This includes: Setting the protocol e.g. HTTP or HTTPS . Setting the domain name or IP address . The setup uses HTTP by default. If you are installing with HTTPS please visit the HTTPS Configuration page. How to Use the set_hostname Script ¶ Required Variable ¶ When running the script you will need the following variable: Hostname : This can be an IP address e.g. 192.168.1.100 or a DNS name e.g. example.com . Steps to Run the Script ¶ Run the Script Use the following commands based on your operating system to execute the set_hostname script: Make sure perl version is 5.34 or never Linux Windows Bash ./config.sh set_hostname PowerShell ./ config . ps1 set_hostname Provide the Hostname When prompted enter the hostname IP or DNS the application. Verify Configuration After running the script ensure that the hostname is correctly configured in the application s settings. Back to top


================================================================================
SECTION 9: https://datatouch.netfein.com/latest/install/6dataskope/
Content Length: 5,320 characters
================================================================================

Dataskope Configuration - Data Touch Skip to content Dataskope Configuration Configuring Data Touch to Connect to Dataskope DAM and Elasticsearch ¶ Before running Data Touch the first time you must configure the connection to: The Dataskope DAM REST API. The Elasticsearch cluster. Use the provided script to set up these configurations and verify connectivity. 1. DAM Configuration ¶ The first step is to configure and test the connection to the Dataskope DAM REST API. Required Variables: ¶ DAM_URL : The URL of the Dataskope DAM REST API. DAM_USERNAME : The username DAM authentication. DAM_PASSWORD : The password DAM authentication. Steps ¶ Run the Script ¶ Use the following command to start the configuration process: Script Command Linux Windows Bash ./config.sh connect_dataskope PowerShell ./ config . ps1 connect_dataskope Enter Configuration Values ¶ The script will prompt you to enter the following values. If you press Enter without providing a value the script will use the default values shown in parentheses: The DAM URL must be entered in the specified format. DAM_URL : Default is https://192.168.185.111:8443 DAM_USERNAME : Default is dev DAM_PASSWORD : Default is VqByiI7tJJZX65uP2jDxVA== Test DAM Connection ¶ The script will test connectivity to the DAM REST API by attempting to authenticate using the supplied credentials. If authentication is successful the script will display a success message: Text Only â Successfully authenticated with DAM endpoint If authentication fails: You will be prompted to retry with different credentials. You can retry up to 3 times before the script exits with an error. Example Output ¶ Below is an example of the script output during the DAM configuration process: Text Only === DAM Configuration === Enter DAM_URL default: https://192.168.185.111:8443 : Enter DAM_USERNAME default: dev : Enter DAM_PASSWORD default: VqByiI7tJJZX65uP2jDxVA== : Testing DAM REST endpoint connectivity... Attempt 1/3 Attempting to obtain authentication token... Response from DAM endpoint: ---------------------------------------- access_token : eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9... token_type : Bearer expires_in : 3600 ---------------------------------------- â Successfully authenticated with DAM endpoint 2. Elasticsearch Configuration ¶ The second step is to configure and test the connection to the Elasticsearch cluster. Required Variables ¶ DAM_ELASTIC_HOSTNAME : The hostname or URL of the Elasticsearch cluster. DAM_ELASTIC_USERNAME : The username Elasticsearch authentication. DAM_ELASTIC_PASSWORD : The password Elasticsearch authentication. Steps ¶ Enter Configuration Values ¶ After completing the DAM configuration the script will prompt you to enter the following Elasticsearch variables. If you press Enter without providing a value the script will use the default values shown in parentheses: DAM_ELASTIC_HOSTNAME : Default is http://192.168.185.111:9200 DAM_ELASTIC_USERNAME : Default is elastic DAM_ELASTIC_PASSWORD : Default is BTb0vkkbYzdM0ejsKYrg Test Elasticsearch Connection ¶ The script will test connectivity to the Elasticsearch cluster by querying the cluster health endpoint. If the connection is successful the script will display a success message: Text Only â Successfully connected to Elasticsearch If the connection fails: You will be prompted to retry with different credentials. You can retry up to 3 times before the script exits with an error. Example Output ¶ Below is an example of the script output during the Elasticsearch configuration process: Text Only === Elasticsearch Configuration === Enter DAM_ELASTIC_HOSTNAME default: http://192.168.185.111:9200 : Enter DAM_ELASTIC_USERNAME default: elastic : Enter DAM_ELASTIC_PASSWORD default: BTb0vkkbYzdM0ejsKYrg : Testing Elasticsearch connectivity... Attempt 1/3 Response from Elasticsearch: ---------------------------------------- cluster_name : elasticsearch status : green number_of_nodes : 3 active_shards : 10 ---------------------------------------- â Successfully connected to Elasticsearch Finalizing the Configuration ¶ Once both the DAM and Elasticsearch configurations are successfully completed the script will automatically update the docker-compose.yml file with the provided values. Example Output ¶ Text Only === Final Configuration === DAM_URL: https://192.168.185.111:8443 DAM_USERNAME: dev DAM_PASSWORD: VqByiI7tJJZX65uP2jDxVA== DAM_ELASTIC_HOSTNAME: http://192.168.185.111:9200 DAM_ELASTIC_USERNAME: elastic DAM_ELASTIC_PASSWORD: BTb0vkkbYzdM0ejsKYrg Updating docker-compose.yml... â docker-compose.yml file has been updated successfully! Updating docker-compose.yml ¶ The following values will be updated in the docker-compose.yml file: DAM_URL DAM_USERNAME DAM_PASSWORD DAM_ELASTIC_HOSTNAME DAM_ELASTIC_USERNAME DAM_ELASTIC_PASSWORD Ensure the docker-compose.yml file is present in the same directory as the script before running it. Error Handling ¶ DAM Connection Errors If authentication with the DAM REST API fails you will have up to 3 retries to enter updated credentials. After the 3rd failed attempt the script will exit with an error. Elasticsearch Connection Errors If the connection to Elasticsearch fails you will have up to 3 retries to enter updated credentials. After the 3rd failed attempt the script will exit with an error. Back to top


================================================================================
SECTION 10: https://datatouch.netfein.com/latest/install/7metadata/
Content Length: 5,218 characters
================================================================================

Metadata Configuration - Data Touch Skip to content Metadata Configuration Metadata Configuration ¶ This documentation explains how to configure Data Focus and SDM as Metadata Services your application. Only one of the metadata servicesâeither Data Focus or SDMâshould be configured. 1. Connect Data Focus ¶ To configure Data Focus as the Metadata Service run the provided script and follow the instructions. Required Variables ¶ Before running the script ensure you have the following variables: Text Only - DATAFOCUS_HOST: The base URL of the Data Focus service. - KEYCLOAK_CLIENT_ID: The Keycloak client ID authentication. - KEYCLOAK_CLIENT_SECRET: The Keycloak client secret authentication. Steps to Configure Data Focus ¶ Run the Script ¶ Use the following command to start the configuration process: Linux Windows Bash ./config.sh connect_datafocus PowerShell ./ config . ps1 connect_datafocus Input Configuration Values ¶ Keycloak is the login mechanism used by Data Focus and the script will prompt you to enter the required variables. If you press Enter without providing a value the default values will be used: DATAFOCUS_HOST : Default is http://localhost KEYCLOAK_CLIENT_ID : Default is datatouch-df-client KEYCLOAK_CLIENT_SECRET : Default is HRqptTY6NY7OuDnLhS8WnyrtxiumuUsy Warning If you have issue with credentials you can set the new value from Keycloak UI on Data Focus. Test Keycloak Authentication ¶ The script will test the Keycloak authentication by requesting an access token using the provided credentials. If the authentication is successful the script will display: Text Only Authentication successful! â If the authentication fails it will display the error response and ask you want to proceed with updating the docker-compose.yml file. Update docker-compose.yml ¶ After confirming the script will update the following values in the docker-compose.yml file: METADATA_SERVICE : Set to DF DATAFOCUS_URL : The URL of the Data Focus service. KEYCLOAK_URL : The Keycloak authentication URL. KEYCLOAK_CLIENT_ID : The Keycloak client ID. KEYCLOAK_CLIENT_SECRET : The Keycloak client secret. AUTH_PROVIDER : Set to DAM METADATA_INTEGRATION : Set to datafocus Example Output ¶ Below is an example of the script output during the Data Focus configuration process: Text Only Enter the Data Focus host default: http://localhost : Enter the Keycloak client ID default: datatouch-df-client : Enter the Keycloak client secret default: HRqptTY6NY7OuDnLhS8WnyrtxiumuUsy : Updated values: METADATA_SERVICE: DF DATAFOCUS_URL: http://localhost KEYCLOAK_URL: http://localhost/auth KEYCLOAK_CLIENT_ID: datatouch-df-client KEYCLOAK_CLIENT_SECRET: HRqptTY6NY7OuDnLhS8WnyrtxiumuUsy Testing Keycloak authentication... Authentication successful! â Do you want to proceed with updating the docker-compose.yml file? y/n : y docker-compose.yml file has been updated! 2. Connect SDM ¶ To configure SDM as the Metadata Service run the provided script. This script also updates the Anomaly and Spectrum service database connections and executes SQL commands on SDM databases. Required Variables ¶ Before running the script ensure you have the following variables: DB_HOST : The hostname or IP address of the database of SDM. DB_PORT : The port number of the database. DB_NAME : The name of the database used by SDM. DB_USERNAME : The username database authentication. DB_PASSWORD : The password database authentication. Steps to Configure SDM ¶ Run the Script ¶ Use the following command to start the configuration process: Linux Windows Bash ./config.sh connect_sdm PowerShell ./ config . ps1 connect_sdm Input Configuration Values ¶ The script will prompt you to enter the required database connection details. If you press Enter without providing a value the default values will be used: DB_HOST : Default is localhost DB_PORT : Default is 5432 DB_NAME : Default is sdmdb DB_USERNAME : Default is sdmuser DB_PASSWORD : Default is sdm123 Execute SQL Commands ¶ The script will execute the multiple sql file on the specified database. This SQL file contains the commands to: - Update the Anomaly and Spectrum service database connections. - Configure SDM as the Metadata Service. Example Output ¶ Below is an example of the script output during the SDM configuration process: Text Only Enter the database host default: localhost : Enter the database port default: 5432 : Enter the database name default: sdmdb : Enter the database username default: sdmuser : Enter the database password default: sdm123 : Updated values: DB_HOST: localhost DB_PORT: 5432 DB_NAME: sdmdb DB_USERNAME: sdmuser DB_PASSWORD: sdm123 Do you want to proceed with running the SQL script? y/n : y SQL script executed successfully! â Important Notes ¶ Discovery Module Requirement Discovery module must be installed on SDM. Ensure that the Discovery module is installed on SDM before running the script. Backup Recommendation It is recommended to back up the SDM database before running the script. Since the script will modify the database structure and data creating a backup is highly recommended. Error Handling If the database connection fails the script will display an error. You can re-run the script with corrected values. Back to top


================================================================================
SECTION 11: https://datatouch.netfein.com/latest/install/8httpsconf/
Content Length: 3,109 characters
================================================================================

HTTPS Configuration - Data Touch Skip to content HTTPS Configuration Enabling HTTPS in Router ¶ The application uses Nginx as a router handling HTTP/HTTPS requests. By default the application runs on HTTP. To enable HTTPS you need to modify the Nginx configuration. Prerequisites ¶ DNS A Records : Properly configured DNS A records pointing to your server s IP address. SSL Certificates : To enable HTTPS you need: A valid SSL certificate .crt file . The corresponding SSL certificate key .key file . Nginx Configuration Directory ¶ The Nginx configuration files are located in the router directory under the application s installation path. This directory is mounted into the Nginx container using the Docker Compose configuration. Example Nginx Configuration HTTPS ¶ Below is an example of the relevant part of the Nginx configuration file. To enable HTTPS uncomment the necessary lines and comment out the HTTP lines : Nginx Configuration File ### UNCOMMENT THE LINES BELOW TO CONVERT APPLICATION TO HTTPS # Redirect all HTTP traffic to HTTPS #server # listen 80 # # Redirect HTTP to HTTPS # 301 https://$server_name$request_uri # server listen 80 default_server # COMMENT THIS LINE IF ENABLING HTTPS # listen 443 ssl default_server # UNCOMMENT THIS LINE TO ENABLE HTTPS # ssl_certificate /etc/nginx/conf.d/test_netfein.crt # UNCOMMENT THIS LINE TO SPECIFY SSL CERTIFICATE # ssl_certificate_key /etc/nginx/conf.d/test_netfein.key # UNCOMMENT THIS LINE TO SPECIFY SSL KEY Steps to Enable HTTPS ¶ Obtain SSL Certificates You need a valid SSL certificate .crt and a private key .key file. Place these files in the router directory e.g. router/your_cert.crt and router/your_cert.key . You must update the certificate lines according to your certificate names on the config file. Modify the Nginx Configuration Open the Nginx configuration file in the router directory. Uncomment the lines HTTPS: listen 443 ssl default_server ssl_certificate and ssl_certificate_key . Comment out the line HTTP: listen 80 default_server . Enable HTTP to HTTPS Redirection Optional - Uncomment the block that redirects HTTP traffic to HTTPS: Nginx Configuration File server listen 80 # Redirect HTTP to HTTPS 301 https:// $server_name$request_uri Restart the Router container to apply the changes: Bash docker-compose restart router Example Workflow ¶ Run the set_hostname script to configure the hostname: Bash ./config.sh set_hostname Update the Nginx configuration to enable HTTPS: Place the SSL certificate and key in the router directory. Modify the Nginx configuration file as described above. Restart the Router service: Bash docker-compose restart router Verify that the application is accessible via HTTPS. Run the following command to start the application. Bash docker compose up -d Notes ¶ Certificate Validity : Ensure that the SSL certificate is valid and trusted by the browser to avoid security warnings. Backup Configuration Files : Before making changes to the Nginx configuration create a backup of the original file. Firewall Settings : Ensure that port 80 and 443 HTTPS is open in your firewall. Back to top


================================================================================
SECTION 12: https://datatouch.netfein.com/latest/install/9deployment/
Content Length: 4,217 characters
================================================================================

Deployment - Data Touch Skip to content Deployment Deploying the application is straightforward. Once you ve completed the configuration use the following steps to start and verify the application. Additional debugging options are provided to help troubleshoot any issues during deployment. Steps to Deploy the Application ¶ 1 . Start the Application Run the following command to start all services in detached mode: Bash docker compose up -d After executing this command you may see some warning messages. These messages are not critical and can be safely ignored. 2 . Verify Container Status Check the status of all containers to ensure they are running and healthy. The all applications will be healty about 3-4 minutes: Bash docker ps If the process takes longer than 5 minutes it may indicate that your CPU does not meet the minimum performance requirements. Look the STATUS column. All containers should display healthy or running . If any container shows unhealthy proceed to the debugging steps below. Pleace proceed to post deployment steps to configure the MINIO. Warnings and Important Notes ¶ Before proceeding with the application deployment please review the following warnings and notes to ensure a smooth and error-free process. Missing Environment Variables ¶ When running Docker Compose you might encounter warnings similar to the following: Text Only The EXTERNAL_LOG_PATH variable is not set. Defaulting to a blank string. The DT_LOG_PATH_NAME variable is not set. Defaulting to a blank string. These warnings indicate that certain environment variables referenced in the docker-compose.yml file are not defined. By default Docker Compose assigns an empty string to these variables. In most cases these warnings can be safely ignored as these variables are intentionally left blank. These values are designed to remain unset unless explicitly required by your application. If the related connections or configurations are working successfully there is no need to modify these variables or address these warnings. Debugging Options ¶ If you encounter issues during deployment use the following commands to identify and resolve problems: 1. View Global Logs ¶ To view logs from all containers managed by the docker-compose.yml file: Bash docker compose logs 2. Monitor Resource Usage ¶ To monitor resource usage CPU memory etc. all running containers: Bash docker stats This is particularly useful identifying containers that are consuming excessive resources. 3. View Specific Container Logs ¶ To view logs a specific container use the following command: Bash docker logs -f container_name - Replace container_name with the name or ID of the container e.g. backend frontend database . - The -f option allows you to follow the logs in real time. 4. Restart a Specific Container ¶ If a container is not responding or is in an unhealthy state restart it: Bash docker restart container_name 5. Check Container Health Status ¶ Inspect the health status of a specific container to understand why it might be unhealthy: Bash docker inspect --format = json .State.Health container_name 6. Check Docker Compose Configuration ¶ Validate the docker-compose.yml file to ensure there are no syntax errors: Bash docker compose config 7. Stop and Remove Containers needed ¶ If you need to stop and completely remove all containers e.g. to clean up before redeployment : Bash docker compose down -v Example Debugging Workflow ¶ Start the application : Bash docker compose up -d Verify container status : Bash docker ps Check logs errors : View global logs: Bash docker compose logs View logs a specific container: Bash docker logs -f container_name Monitor resource usage : Bash docker stats Restart an unhealthy container : Bash docker restart container_name Inspect health status : Bash docker inspect --format = json .State.Health container_name Rebuild and restart containers necessary : Bash docker compose up -d --build Notes ¶ Health Checks : Ensure that health checks are correctly configured in the docker-compose.yml file all services. This helps Docker determine the health status of containers. Container Names : Use docker ps to list all running containers and identify their names or IDs. Back to top


================================================================================
SECTION 13: https://datatouch.netfein.com/latest/install/10monitoring/
Content Length: 3,692 characters
================================================================================

Monitoring - Data Touch Skip to content Monitoring The application includes a fully provisioned and preconfigured monitoring stack that allows you to monitor container performance resource usage and logs. This stack is ready to use immediately after installation and is accessible via the monitoring URL. Monitoring Stack Overview ¶ The monitoring stack consists of the following tools: cAdvisor : Collects container-level metrics such as CPU memory and network usage. Promtail : Collects logs from Docker containers and forwards them to Loki. Loki : A log aggregation system storing and querying logs. Prometheus : A time-series database collecting metrics from cAdvisor and other sources. Grafana : A visualization and dashboard tool to view metrics and logs. Key Features ¶ Container Status : View the status of all containers in the application. Resource Usage : Monitor CPU memory and network usage each container. Log Aggregation : Access centralized logs all containers. Log Search and Error Detection : Search logs specific terms e.g. ERROR to quickly identify issues. Accessing the Monitoring Dashboard ¶ URL : The monitoring stack is available at: Text Only http://$hostname/monitor/ Replace $hostname with the hostname or IP of your server. Default Grafana Credentials : Username : admin Password : admin Security Note For security purposes it is recommended to change the default credentials after the first login. Preconfigured Dashboards ¶ The monitoring stack comes with preconfigured dashboards in Grafana ready to use out of the box: Container Metrics Dashboard : Displays CPU memory and network usage all containers. Helps identify resource bottlenecks or underutilized containers. Log Dashboard : Aggregates logs from all containers. Allows searching specific terms e.g. ERROR WARNING to troubleshoot issues. Overview Dashboard : Provides a high-level summary of container health and application performance. How to Use the Monitoring Tools? ¶ 1. View Container Metrics ¶ Navigate to the Docker Monitoring Dashboard in Grafana. Select the specific container you want to monitor from the dropdown menu. View the container s status UP/DOWN and uptime duration. Analyze CPU memory and disk I/O usage through the time-series graphs. Use the time range selector to focus on specific time periods e.g. last 30 minutes last hour . 2. Search Logs ¶ Use the Log Tail panel to view the most recent log entries. Filter logs by log level INFO WARN ERROR to focus on specific types of events. Use the search bar to filter logs by keywords or specific service names. Check log counts Info Warn Error to quickly identify potential issues. 3. Detect Errors ¶ Monitor the Error Count metric unexpected increases. Review WARN and ERROR log entries in the Log Tail section. Identify patterns in error messages such as repeated warnings about vulnerability alerts. Set up alerts in Grafana optional to notify you when specific log patterns are detected. Deployment Details ¶ Network Configuration The monitoring stack uses a Docker network and does not expose any services directly to the host. All components communicate internally within the Docker network. Preconfigured Setup The stack is fully provisioned and preconfigured during installation. No additional setup is required after deployment. Security Recommendations ¶ Change Default Credentials : After the first login update the default Grafana credentials admin:admin to a secure username and password. Restrict Access : Ensure that access to the monitoring stack is restricted to authorized users only. Monitor Logs Security Issues : Regularly search logs unusual activity or errors that could indicate security issues. Back to top


================================================================================
SECTION 14: https://datatouch.netfein.com/latest/install/11post/
Content Length: 2,780 characters
================================================================================

After Deployment - Data Touch Skip to content After Deployment MinIO Bucket Creation ¶ MinIO is used as a secure and scalable object storage solution. Accessing MinIO ¶ URL : MinIO can be accessed at: Text Only http://$hostname/minio/ or HTTPS is enabled: Text Only https://$hostname/minio/ Replace $hostname with the hostname or IP address of your server. Login Credentials : Use the credentials provided during the MinIO setup to log in. Required Buckets ¶ Please verify the existence of the following buckets in MinIO: datatouch doc-gro These buckets are essential storing application data. Note: If any of these buckets do not exist please create them beforing proceeding. If the datatouch and doc-gro bucket already exists you can skip their creation and continue with the next steps. Steps to Create Buckets in MinIO ¶ Log in to MinIO : Open your browser and navigate to either: http://$hostname/minio/ or https://$hostname/minio/ HTTPS is enabled . Enter your MinIO username and password to log in. Defaults are username: admin password: uguV3SLiUH Create a Bucket : Once logged in click the + button or the Create Bucket option in the MinIO interface. Enter the bucket name as datatouch . Click Create to save the bucket. Create Another Bucket : Repeat the process to create a second bucket. Enter the bucket name as doc-gro . Click Create to save the bucket. Verify the Buckets : Ensure both buckets datatouch and doc-gro are listed in the MinIO interface. The installation process is now complete. You can now navigate to the Logging in to Data Touch page to learn how to access Data Touch and begin using it effectively. Notes ¶ Bucket Names : Ensure the bucket names are entered exactly as specified datatouch and doc-gro to avoid configuration issues. Access Control : If required configure access policies the buckets to restrict or allow access based on your application s needs. HTTPS Configuration : If MinIO has been configured to use HTTPS ensure that the necessary certificates are installed and that you use the https:// protocol in the URL. Troubleshooting ¶ If you encounter issues creating buckets consider the following steps: Check MinIO Service : Ensure the MinIO service is running: Bash docker ps Look the MinIO container in the list of running containers. View MinIO Logs : If the service is running but inaccessible check the MinIO logs: Bash docker logs -f minio_container_name Replace minio_container_name with the name of the MinIO container. Verify Network Access : Ensure that the $hostname/minio/ URL is accessible from your browser. Recreate Buckets : If buckets are not listed recreate them using the steps above. Check HTTPS Configuration : If using HTTPS verify that the SSL/TLS certificates are correctly configured and valid. Back to top


================================================================================
SECTION 15: https://datatouch.netfein.com/latest/install/troubleshooting/
Content Length: 1,173 characters
================================================================================

Troubleshooting - Data Touch Troubleshooting Issue Solution Misconfigured Application - Reinstallation Required 1. Navigate to the datatouch folder: cd datatouch 2. Shut down and remove all services: docker compose down -v 3. Delete the entire datatouch folder.To clean up the entire Docker environment optional : docker system prune -af Cannot Access User Interface / CORS Errors If you cannot reach the user interface or encounter CORS errors: 1. Restart the router container: docker compose restart router 2. Verify the hostname configuration as described in Hostname Configuration. 3. Ensure ports 80 and 443 are open on the server. Compose is Not a Docker Command If you encounter the error compose is not a docker command: This likely indicates that Docker Compose is not properly installed or configured. Revisit the installation steps to ensure Docker Compose is correctly set up on your system. Invalid Parameter: redirect_uri If you receive an Invalid Parameter: redirect_uri error: 1. Double-check the configurations outlined in Keycloak Configuration . 2. Verify that the Root URL and Valid Redirect URIs are properly set according to your hostname. Back to top


================================================================================
SECTION 16: https://datatouch.netfein.com/latest/usage/1logging/
Content Length: 801 characters
================================================================================

Logging in to Data Touch - Data Touch Logging in to Data Touch The HTTP/HTTPS selection made during the installation process of Data Touch DAM affects the logging link. Relevant links are given below. Host IP is the public IP of the installed machine. If HTTP is preferred during the installation process connect to Data Touch DAM from http:// : /#/login port is 8445 by default . If HTTPS is preferred during the installation process connect to Data Touch DAM from https:// host_IP/#/login. To create a user go to Dataskope Console Settings Users . Click + New enter User Name Password check the Use Data Touch from Account Information and save the user. After creating user follow the steps given below. 1. Enter User Name and Password that you set in Dataskope. 2. Click Sign in button. Back to top


================================================================================
SECTION 17: https://datatouch.netfein.com/latest/usage/2menucontrols/
Content Length: 1,080 characters
================================================================================

Menu and Controls - Data Touch Menu and Controls Menu Function Dashboard Used to view the dashboard. Sources Used to view sources. Clients Used to view clients. Heatmaps Used to create heatmaps. Data Lineage Used to create data lineage diagram. Radar Used to list UCMDB configurations. Anomaly Detection Used to detect anomalies. Includes Anomaly Dasboard and Scenario List submenus. Risk Overview Used to view general risk overview. Includes Risk Dashboard Data Source Risk and User Risk submenus. Spectrum Reports Used to view report details and edit existing reports. Integration Used to integrate Data Touch with specified applications such as Logstash Jira MS Teams and Mail . Settings Used to manage Settings. Includes Matching Data Source Risk Report Parameters UCMDB-Config and Source Setting submenus. Used to close and open the menu on the left. The menu part can be closed a wider graphic view. Used to select the interface language. There are Turkish and English options. Used to reach user information product details and the User Guide . Used to log out. Back to top


================================================================================
SECTION 18: https://datatouch.netfein.com/latest/usage/3dashboard/
Content Length: 2,508 characters
================================================================================

Dashboard - Data Touch Skip to content Dashboard When signed in Data Touch DAM the Main Dashboard is opened as default. All existing sources within the environment are viewed on the Main Dashboard. Users see the sources depending on their role . They can quickly understand the distribution of the columns sensitive columns sensitive types and consumers . In the main dashboard the sources are displayed on the pie chart according to their size depending on the total column the total number of sensitive columns etc. . As well as the first 10 sources are shown as bars with the count of sensitive types sensitive columns and non-sensitive columns. Charts ¶ Data Touchâs main dashboard provides the Distribution of Columns Distribution of Sensitive Columns Distribution of Sensitive Types and Distribution of Consumers pie charts. Data Source numbers are limited to ten to show on the main dashboard within pie charts and bars. Charts of the Main Dashboard are given below with related information. Distribution of Columns shows the total number and distribution of columns in every data source that comes from Discovery. Distribution of Sensitive Columns shows the total number and distribution of columns with sensitive content. Distribution of Sensitive Types shows the total number and distribution of sensitive types as date of birth credit card information etc. Distribution of Consumers shows the total number and distribution of consumers who send a query the data by specifying a time frame. Filtering Sources on the Charts ¶ When users want to extract source from the pie chart they can click on the name of the source in expanded mode. The extracted source remains passively visible. To be included the source name is clicked again and the relevant source is added to the pie chart. Users can view the exact distribution and number of a source on the pie chart by hovering over any slice. Date Filter ¶ The date range some charts can be customized with date filter options. Users can access 1 month 3 months 6 months and Custom Date options from the date drop-down menu of the Distribution of Consumers graph. When the Custom Date option is clicked Start date and End date options are displayed the user to enter. Sources List ¶ Sources List contains the 10 most recently scanned sources . It presents Sensitive and Non-Sensitive Column values via horizontal bar graphs. It also shows the Sensitive Type value of the source. Users can click on More to see the details of all sources. Back to top


================================================================================
SECTION 19: https://datatouch.netfein.com/latest/usage/4sources/
Content Length: 3,951 characters
================================================================================

Sources - Data Touch Skip to content Sources Users can reach all sources as a list by clicking the Sources button on the menu. Through this screen users can search by filtering sources. The source to be filtered is selected from the Sources Search drop-down menu. Next the preferred Last Scan Date Start Date End Date Sensitive Type Count Min Max Sensitive Column Count Min Max Non-Sensitive Column Count Min Max and Total Column Count Min Max parameters are filled. As values are entered into the parameters the results are filtered and the list is updated. By clicking on the EXPORT button users can export the filtered list to Excel. Source Dashboard redirects to the Source Dashboard page which contains a detailed analysis of the relevant source. Source Heatmap redirects to the Heatmaps menu where detailed heatmaps can be created. Source Dashboard ¶ Source Dashboard includes detailed information about the selected source. It shows graphics the analysis of all scanned columns and tables of the source based on sensitive and non-sensitive data types and consumers who have accessed to these data . Sensitive Types ¶ This chart shows all sensitive types in terms of their names and numbers. Consumer Related Charts ¶ This section includes the Daily/Weekly/Monthly/Yearly Usage Number of Consumers and Total Query of Consumers charts. The general date filtering field affects these three charts together. Additionally an extra date filter is available the Daily/Weekly/Monthly/Yearly Usage chart. The general date filter affects what options are presented in the specific date filter in this chart. Daily/Weekly/Monthly/Yearly Usage This line chart shows total usage by date. The date range can be changed via the filter. Number of Consumers This bar chart shows the number of consumers who accessed each sensitive type . Users can see the exact number of consumers by waiting a on any of the bars that they want to see the detail. In addition users can hide the sensitive types that have no access by the consumers in each time interval by selecting the Hide Zero Values. â Also after seeing the number of consumers of any sensitive type users can see the consumer details by clicking on a bar. Consumersâ CLIENT CLIENT IP and DATABASE USER information can be accessed from the opened window. Total Query of Consumers This bar chart shows the total query number of consumers who accessed sensitive types. Users can see the exact number of queries on the bars according to Client IP or Client Hostname . Client IP or Hostname selection is made with the toggle. Data Usage ¶ By clicking Data Usage button on the Source Dashboard users can reach out to the data usage section with DAM data which indicates the access information in terms of query details. On this page database information of the given source environment will be listed in terms of Database Name Schema Name Table Name Columns and Sensitive Types . It also shows whether the columns are a sensitive type or not. Users can filter all columns of the list and reach the filtered data on the page. By clicking EXPORT users can export the filtered data to Excel. When is clicked on Data Usage page Select Dates and Action window shows up. Users need to select Date frame that they want to filter the query data with the options of a last 1 Month 3 Months 6 Months or Custom Date interval. Also users can select Actions query types otherwise application gets all actions without filtering. Depending on the Date and Actions selection detailed information of the queries that are performed by the consumers within the specific time frame will be displayed as a list on this page. By clicking EXPORT users can export the filtered data to Excel. By clicking as button users can reach all query details that are executed by the specific consumers as a list. By clicking EXPORT users can export the filtered data to Excel. By clicking button user can copy the relevant query. Back to top


================================================================================
SECTION 20: https://datatouch.netfein.com/latest/usage/5clients/
Content Length: 1,694 characters
================================================================================

Clients - Data Touch Skip to content Clients Users can access the DB and OS users in a list through the relevant tabs by clicking the Clients button in the menu. Through this screen users can search by filtering clients. Username Database Server Database Port and Date parameters can be used to filter. As values are entered to the parameters the results are filtered and the list is updated. By clicking EXPORT users can export the filtered data to Excel. Client Dashboard redirects to the Client Dashboard page which contains a detailed analysis of the relevant client. Client Dashboard ¶ Client Dashboard includes detailed information on the selected client. It shows graphics the analysis of all scanned columns and tables of the client based on usage and queries . Date range can be used as filter. Daily/Weekly/Monthly/Yearly Usage This chart shows client usage according to the selected date range. Total Query Numbers This chart shows the total query number by specifying the query types. User Life Cycle ¶ By clicking the LIFE CYCLE button on the Client Dashboard users can reach out to the user life cycle with Granted By Action Detail and Time details. Client Usage ¶ By clicking the CLIENT USAGE button on the Client Dashboard users can reach out to the client usage section with the Database Schema Name Table Name and Column details. â Column queries ¶ By clicking column queries button users can reach all column query details as a list. By clicking EXPORT users can export the filtered data to Excel. Table queries ¶ By clicking table queries button users can reach all table query details as a list. By clicking EXPORT users can export the filtered data to Excel. Back to top


================================================================================
SECTION 21: https://datatouch.netfein.com/latest/usage/6heatmaps/
Content Length: 1,804 characters
================================================================================

Heatmaps - Data Touch Skip to content Heatmaps Heatmaps page can be accessed directly from the menu or any source row on the Sources page. There are four types of heatmaps: Table Query Count Heatmap Client Count Heatmap Table Action Type Query Count Heatmap and Sensitive Type Time Heatmap . The user must select one source and one of the heatmaps at the very beginning of the analysis. Table Query Count Heatmap ¶ This heatmap shows the total query number each table a specific date. Users can filter out the Schema Names of the project Table names each schema Period timeframe and Date Range . Each heatmap type has its filter selection options which can be found in the following sections. After completing the selections users click CREATE HEATMAP button below to initiate the creation process of heatmaps. Once the application creates a heatmap it will show up directly on the page. Client Count Heatmap ¶ This heatmap shows the total query number accessed from each client to each table. Users can filter out the Clients by Name Schemas Names of the source Tables each schema and Date interval . Once the application creates a heatmap it will show up directly on the page. â Table Action Type Query Count Heatmap ¶ This heatmap shows the total query count number by action type select update delete etc. each table. Users can filter out the Actions by Type Schema Names of the source Tables each schema and Date interval . Once the application creates a heatmap it will show up directly on the page. â Sensitive Type Time Heatmap ¶ This heatmap shows the total query number within the selected period each sensitive type . Users can filter out the Sensitive Types Period timeframe and Date intervals . Once the application creates a heatmap it will show up directly on the page. â Back to top


================================================================================
SECTION 22: https://datatouch.netfein.com/latest/usage/7datalin/
Content Length: 1,540 characters
================================================================================

Data Lineage - Data Touch Skip to content Data Lineage Data Lineage creates a map that shows all the stages changes and relationships a table column or client has gone through starting from its source. Provides the ability to track the ways data is created used modified and transferred . It aims to provide the user with a detailed understanding of the origin use and changes of data in data management processes. Table Lineage ¶ To create a Table Lineage map select schema name table and date then click Get Visualization . The relationship between the schema and tables is displayed in the created map. In the list given under the map Client Host Name Query and Time Created details are displayed. By clicking the Client Lineage button you can view detailed information about the clients in the Client Lineage chart. Column Lineage ¶ To create a Column Lineage map select schema name table columns and date then click Get Visualization . The relationship between the schema and columns is displayed in the created map. In the list given under the map Client Host Name Query and Time Created details are displayed. By clicking the Client Lineage button you can view detailed information about the clients in the Client Lineage chart. Client Lineage ¶ To create a Client Lineage map select the client schema name and date then click Get Visualization . The relationship between the client and schema is displayed in the created map. In the list given under the map Client Host Name Query and Time Created details are displayed. Back to top


================================================================================
SECTION 23: https://datatouch.netfein.com/latest/usage/8radar/
Content Length: 456 characters
================================================================================

Radar - Data Touch Radar Radar integrates with Opentext UCMDB to discover databases within the network and generate reports. It identifies which databases are monitored by the DAM agent and which are not and helps detect unauthorized database instances. It is used to list UCMDB configurations . By clicking on button users can reach UCMDB Discovery Results . Users can filter the results and export them to Excel by clicking the EXPORT button. Back to top


================================================================================
SECTION 24: https://datatouch.netfein.com/latest/usage/9anomaly/
Content Length: 2,404 characters
================================================================================

Anomaly Detection - Data Touch Skip to content Anomaly Detection Anomaly Detection identifies patterns or situations within the data set that deviate from the norm . It aims to detect unusual behavior outliers or potential threats that may indicate bugs security breaches or performance issues . Anomaly Dashboard ¶ Anomaly Dashboard provides an overview of detected anomalies. It includes Total Anomaly Numbers Daily/Weekly/Monthly Anomaly Detection graphs and a detailed Anomaly List . Total Anomaly Numbers ¶ This chart shows the total anomaly numbers the scenarios. It offers the opportunity to choose date ranges 1 3 or 6 months and scenarios. It shows the exact numbers above the bars of the scenarios. Daily/Weekly/Monthly Anomaly Detection ¶ This chart shows the number of anomalies detected on a date -based basis. The date axis can be arranged as Weekly and Monthly . Detected Anomaliesâ List ¶ In this list detected anomalies are shown with their scenario detected date information alert count and accuracy rate details. Column & Value : It shows the column and value of the detected anomaly. Statistics : It shows the accuracy rate value and impressive parameters of the detected anomaly. Full Information : It shows all information about the detected anomaly. Actions Denied : It is used to deny the detected anomaly. Decision : It is used in cases where the detected anomaly cannot be denied or approved . Approved : It is used to approve the detected anomaly. Scenario List ¶ This list contains the scenarios with their details. * New Scenario: It is used to create a new scenario. A new scenario is created by filling in the Scenario Definition Scenario Name Anomaly Precision Level Training Algorithm Training Period Select and Where fields and click Save . Anomaly detection templates provide a base most common anomaly detection scenarios. please do not hesitate to customize and save the anomaly detection scenarios better focused results. Delete: It is used to delete the scenario. Edit: It is used to edit the scenario. History : It is used to display history details of the scenario. Update Retention Period : It is used to change the retention period. The system will store the data until the retention period days set arrives. The older data will be deleted after the specified time. Stop : It is used to stop the training. Clone : It is used to clone the scenario. Back to top


================================================================================
SECTION 25: https://datatouch.netfein.com/latest/usage/10risk/
Content Length: 964 characters
================================================================================

Risk Overview - Data Touch Skip to content Risk Overview Risk Dashboard ¶ Data Source Risk and User Risk analysis are presented with a general perspective and date filtering option. All data sources are listed according to risk score and the six with the highest risk score are shown in the dashboard. Click View all data sources to see all data sources with details. All users are listed according to risk score and the six with the highest risk score are shown in the dashboard. Click View all users to see all data sources with details. â Action Details When you click on the information icon from Actions column of any data source or user the details page appears. Data Source Risk ¶ It contains risk analysis only Data Sources . It can be sorted in ascending or descending order according to the risk score. â User Risk ¶ It contains risk analysis only Users . It can be sorted in ascending or descending order according to the risk score. â Back to top


================================================================================
SECTION 26: https://datatouch.netfein.com/latest/usage/11spectrep/
Content Length: 1,434 characters
================================================================================

Spectrum Reports - Data Touch Spectrum Reports It is used to define new reports or access existing reports. It provides a detailed list of existing reports and it is possible to search with various filtering options. Edit : It is available the reports with a status of fail or success and used to edit the spectrum report. After the necessary changes are made click Save to apply them. To define a new report follow these steps: 1. Click +NEW button from the right top corner of the My Reports page. 2. Fill in the Report Name Description and Schedule Time fields. 3. Choose the Active/Passive option. 4. Select the Report Type and click Next . 5. On Data Source tab choose the Data Source . 6. Make your selection Notify by Email and Retention Period . 7. Click Save . Report History : When the button from Actions column of any report is clicked the report generation times that report are listed. Report Detail : When you click on the button from Actions column the name data source IP address port grantee and privilege information of the relevant report is displayed. By clicking EXPORT users can export the filtered data to Excel. Result information : When you click on the button from Actions column the name IP address port and log information of the relevant report is displayed. Export Report : When you click the EXPORT button on the left of the Report Detail the relevant report is downloaded as an Excel file. Back to top


================================================================================
SECTION 27: https://datatouch.netfein.com/latest/usage/12integration/
Content Length: 2,609 characters
================================================================================

Integration - Data Touch Skip to content Integration Data Touch can integrate with some applications. The Integrations menu is where these integrations are managed and contains the Integration Events and Templates tabs. Integration Tab ¶ It is used adding and managing integrations. Add Integration field shows the available integrations adding. My Integrations field lists the added integrations. The integration status can be adjusted using the Active/Passive toggle. To add an integration click the + button at the top right of the relevant integration. After filling in the required fields the connection can be tested with the Test Connection button. Then click the + Add Integration button. Events Tab ¶ Events are the core components of the integration module. Each event is linked to a specific action which defines the integration process. For example a Spectrum Reports event can be configured to trigger an E-Mail action upon execution. The Events tab allows users to create new events and manage existing ones. With the CHILDREN LIST button child events of the event are displayed. By clicking the + button next to the Children List button a new child event can be added. To add a new event click the + ADD button in the top right corner. Fill in the Event Key Event Name and Description fields in the pop-up window then click +ADD EVENT . The button is used to add event data . Event data can be added separately to both events and child events. Existing event data can be edited or deleted from the Actions section. With the Copy Key button in the Actions section the key of the event data can be copied. Templates Tab ¶ Templates are the message bodies tied to the desired integrations. Any triggered event will be binded with the corresponding template and then delivered with the desired integration. For example when a Spectrum Reports event is triggered the event is added into the specified e-mail template including the desired message body and the information regarding the event and even the report as an attachment and then sent via the e-mail system. There are two ways to create a new template: by cloning an existing template or by creating a new one from scratch. To clone a template click the button in the Actions section of the existing template. The pop-up window will display the template s properties which can be modified or left unchanged. Then click the Add Template button. To create a template from scratch click the +ADD button in the top right. Fill in the Template Name Integration Type Event Type and Templates fields then click the +ADD TEMPLATE button. Back to top


================================================================================
SECTION 28: https://datatouch.netfein.com/latest/usage/13settings/
Content Length: 6,901 characters
================================================================================

Settings - Data Touch Skip to content Settings Matching ¶ This page includes four tabs: Environment-Host Matching View-Table Mapping Client-Schema Mapping and Sensitive Type Templates . Environment-Host Matching ¶ The IPâs coming from Discovery and DAM is not displayed as one even they are the same. To match this information together users should match the environments and hostnames together. By clicking on + NEW button users can add a new match between environments and hostnames to the system. To add a new match choose Environments and Hostnames information in the opened window and click the Save button. To remove the existing match click the trash can . View-Table Mapping ¶ By clicking on + NEW button users can add a new map between view and table to the system. To add a new map enter View Schema View Table and View Column fields. Then choose Schema Table and Column information in the opened window and click the Save button. To remove the existing match click the trash can . To edit the existing match click the Edit icon. Client-Schema Mapping ¶ By clicking on + NEW button users can add a new map between the client and schema to the system. To add a new map choose Client and Database information . Then enter the Schema field in the opened window and click the Save button. To remove the existing match click the trash can . To edit the existing match click the Edit icon. Sensitive Type Templates ¶ By clicking on + NEW button users can add a new sensitive type. To add a new sensitive type enter the Type name and select the Classifications . Then click the Save button. To remove the existing type click the trash can . To edit the existing type click the Edit icon. By clicking button from the Sensitive Type Templates tab of the Matching menu users can select the data source the desired sensitive type templates. â Data Sources ¶ It is used to manage data sources. It contains the Data Source and Data Source Typ e tabs. Data Source ¶ It is used to add new data sources and list existing ones with details. To remove the existing data source click the trash can . â Edit Data Source To edit any data source click on the Edit button in the action section of the relevant data source. After the necessary updates are made the connection is tested with the Test Connection button and the changes made are saved with the Save button. â New Data Source Definition To define a Data Source click on the +NEW button at the top left of the Data Sources . Fill in the Source Name Data Source Type DF Data Source Database Name Description Database Version Username Password Hostname Server IP and Server Port fields in the Data Source Definition window. Then click the Save button. â Data Source Type ¶ It is used to list database types. To remove the existing data source type click the trash can . To edit the existing data source type click the Edit icon. Risk ¶ This page can be accessed by clicking Risk button under the Settings on the menu. It is used to access all details and settings related to risk. It contains Data Source Users Data Source Risk User Risk Risk Label System Parameters and Parameter Assignment tabs. Data Source Tab ¶ Risk Score Calculation selection can be made data sources based on the given data source list. Users Tab ¶ Risk Score Calculation selection can be made users based on the given users list. Data Source Risk Tab ¶ It lists the risks that may occur in data sources with details. Edit Data Source Risk To edit any data source risk click on the Edit button in the ACTIONS section of the relevant data source risk. Risk can also be calculated by entering parameters. For example start and end dates can be selected and values can be entered risk calculation. After the necessary updates are made they are saved with the Save button. â Adjust Risk Score It is used to adjust data source risk scores. For given risks the Risk Score Weight value can be changed and the active/passive status of the risk can be selected. After the necessary changes are made they are saved with the Save button. The sum of the risk score values must be one hundred . User Risk Tab ¶ It lists the risks that may occur in users with details. â Edit User Risk To edit any user risk click on the Edit button in the ACTIONS section of the relevant user risk. After the necessary updates are made they are saved with the Save button. Score Weight Edit It is used to adjust score weight. For given risks the Risk Score Weight value can be changed and the active/passive status of the risk can be selected. After the necessary changes are made they are saved with the Save button. The sum of the risk score values must be one hundred. â Risk Label Tab ¶ It shows risk labels their types and min-max values. Edit Risk Label It is used to change the max values of risk labels . After the necessary changes are made they are saved with the Save button. System Tab ¶ It lists the risks that may occur in system with details. â Edit System Risk It is used to change the running period of the relevant system risk. After the necessary change is made it is saved with the Save button. Parameters tab ¶ It is used to list and manage risk parameters. To remove the existing risk parameter click the trash can . To edit the existing risk parameter click the Edit button. To create a new Risk Parameter click +NEW button. Enter the Name Description Type and Default Value fields and click Save . Parameter Assignment Tab ¶ It is used to assign risk parameters. To assign a risk parameter click button from the Actions. In the opened window choose Parameter and click ADD button. â Report Parameters ¶ Parameters Tab ¶ It is used to list and manage report parameters. To remove the existing report parameter click the trash can . To create a new Report Parameter click +NEW button. Enter the Name Type and Description fields and click Save . To edit the existing report parameter click the Edit icon. After the necessary change is made it is saved with the Save button. â Assignment tab ¶ It is used to assign report parameters. To assign a report parameter click button from the Actions . In the opened window choose Parameter enter Default Value and click SAVE button. â UCMDB Config ¶ It used to list UCMDB Configurations. Deletion and editing operations are performed from the Actions section. The button redirects to the UCMDB Discovery Results page detailed review. To create a new UCMDB Definition click +NEW button. Enter the Name Description Base API URL User Name Password Repository Client Context and Customer Name fields and click Save . â Source Setting ¶ It is used to list existing sources and manage their enabled or disabled status. Health Checks ¶ The Health Check page provides a real-time overview of the system s operational status. It monitors key components and services to ensure they are running correctly and efficiently. Back to top


================================================================================
SECTION 29: https://datatouch.netfein.com/latest/riskgui/
Content Length: 4,075 characters
================================================================================

Risk Overview - Data Touch Skip to content Risk Overview Databases are the main storage of actionable business information. Alongside performance security is a major concern to continue practicing the business-as-usual activities. A key concern to visualize and often control is the privileges of the users defined in these databases. Having such information provides extensive leverage against security and compliance concerns. The number of users with administrative rights unusual access times multiple login attempts in a defined time range and many more aspects of information would provide actionable data to take corrective actions needed. This page describes the Data Touch risk details. To view the risk dashboards navigate to the Risk Overview menu on the left side of the Data Touch interface. Refer to Risk Overview and Risk Settings sections detailed instructions on using the risk calculation feature. The table below lists the risk types and categories in Data Touch DAM . Risk Type Categories DATA SOURCE RISK EVENT_IN_RISK DATA SOURCE RISK DATA_IN_RISK DATA SOURCE RISK USER_IN_RISK USER RISK PRIVILEGE_IN_RISK USER RISK EVENT_IN_RISK SYSTEM RISK DATASOURCE_USERS_TRANSFER_FROM_REPORT SYSTEM RISK RISK_SCORE_CALCULATION Data Source Risk ¶ Data Source-focused risk calculations are given in the table below. CATEGORY DEFINITION DETAIL EVENT_IN_RISK Suspicious character usage Number of suspicious characters defined by the administrator used in the database EVENT_IN_RISK Nonstandard access and login times Number of connections in the defined off-work hours EVENT_IN_RISK Exceptional volume of SQL error Number of SQL error messages in the database EVENT_IN_RISK Suspicious client connection Number of applications consuming the database EVENT_IN_RISK Suspicious data access on critical datasets The number of SELECT commands sent to the columns with sensitive tagged metadata EVENT_IN_RISK Suspicious data change on critical datasets The number of INSERT UPDATE and DELETE commands sent to the columns with sensitive tagged metadata DATA_IN_RISK Repeated failed login Number of failed attempts to login per database DATA_IN_RISK Sensitive data count Number of sensitive information in the environment tagged by metadata USER_IN_RISK System user count Number of users with non-administrative privileges in the database USER_IN_RISK Number of users accessing critical data Number of database users who have the privilege to send SELECT command to the columns with sensitive tagged metadata USER_IN_RISK Admin user count Number of users with administrative privileges in the database User Risk ¶ User-focused risk calculations are given in the table below. CATEGORY DEFINITION DETAIL PRIVILEGE_IN_RISK Number of system privileges Number of system privileges table/object assigned per user PRIVILEGE_IN_RISK Number of granted roles Number of roles assigned per user EVENT_IN_RISK Unexpected error on connection start Number of connection failures to the database EVENT_IN_RISK Suspicious data access on critical datasets The number of INSERT UPDATE and DELETE commands sent to the columns with sensitive tagged metadata per user EVENT_IN_RISK Repeated failed login Number of failed login attempts per user per database EVENT_IN_RISK Excessive Data Modifications The number of INSERT UPDATE and DELETE commands sent to the columns per user EVENT_IN_RISK Suspicious data change on critical datasets The number of INSERT UPDATE and DELETE commands sent to the columns with sensitive tagged metadata per user EVENT_IN_RISK Exceptional volume of SQL error Number of SQL error messages in the database per user System Risk ¶ System-focused risk calculations are given in the table below. CATEGORY DEFINITION DETAIL SYSTEM Used to retrieve user information from the Data Source using the report application Used to retrieve user information from the Data Source using the report application SYSTEM Used to risk score calculation user Used to risk score calculation user SYSTEM Used to risk score calculation database Used to risk score calculation database Back to top


================================================================================
SECTION 30: https://datatouch.netfein.com/latest/spectreports/reportgui/
Content Length: 1,741 characters
================================================================================

Introduction - Data Touch Skip to content Introduction This page provides the Data Touch data source spectrum report types with features and sample outputs. To view reports navigate to the Reports menu on the left side of the Data Touch interface. Refer Spectrum Reports and Report Parameters section in the Usage detailed instructions on using the reports feature. Data Source Spectrum Reports ¶ The table below lists the names and data source types of the data source spectrum reports available in Data Touch. Data Source Type Name MONGODB All User and Admin Count MONGODB User Granted Role and Table Count MONGODB User Privilege Report MSSQL All User and Admin Count MSSQL User Grants and Privileges MSSQL Databases MSSQL User Privilege Report MSSQL Login List MSSQL User Granted Role and Table Count MYSQL All User and Admin Count MYSQL User Privilege Report MYSQL User Granted Role and Table Count ORACLE Object Privileges ORACLE User Privilege Report ORACLE User Granted to Become User ORACLE User Granted to Alter System ORACLE All System Privileges and Admin Options ORACLE Roles Granted ORACLE User Granted Role and Table Count ORACLE All Admin User Roles ORACLE All User ORACLE All User and Admin Count POSTGRESQL All User and Admin Count POSTGRESQL User Privilege Report POSTGRESQL User Granted Role and Table Count SAP HANA All User and Admin Count SAP HANA User Privilege Report SAP HANA User Granted Role and Table Count VERTICA All User and Admin Count VERTICA User Privilege Report VERTICA User Granted Role and Table Count COUCHBASE All User and Admin Count COUCHBASE User Privilege Report ELASTICSEARCH All User and Admin Count ELASTICSEARCH User Privilege Report ELASTICSEARCH User Granted Role and Table Count Back to top


================================================================================
SECTION 31: https://datatouch.netfein.com/latest/spectreports/mongo/
Content Length: 1,609 characters
================================================================================

MongoDB - Data Touch Skip to content MongoDB Data Source Spectrum Reports ¶ The reports All User and Admin Count User Granted Role and Table Count and User Privilege Report can be generated the MongoDB data sources. The following permissions are required the following MongoDB objects: read on admin database listDatabases on admin database find on system.users collection in each database find on system.roles collection in each database listCollections on all databases These permissions are necessary the methods in the various classes within DatabaseRepo . The code uses MongoDB commands and operations to retrieve user role and database information. These reports are explained in detail below. All User and Admin Count ¶ The report provides a summary of the number of admin and non-admin user accounts in a MongoDB database along with details about the data source and a timestamp indicating when the data was recorded. It is useful assessing the distribution of administrative and non-administrative users within the database. User Granted Role and Table Count ¶ The report provides information about user accounts in a MongoDB database including the number of roles granted to each user and the data source details. It is useful tracking user privileges and understanding the structure of user roles within the database. User Privilege Report ¶ The report provides details about various users and roles within a MongoDB database including the number of roles granted user IDs and any associated role names. It is useful managing and auditing user permissions and roles in the database. â Back to top


================================================================================
SECTION 32: https://datatouch.netfein.com/latest/spectreports/mssql/
Content Length: 2,527 characters
================================================================================

MSSQL - Data Touch Skip to content MSSQL Data Source Spectrum Reports ¶ The reports All User and Admin Count User Grants and Privileges Databases User Privilege Report Login List and User Granted Role and Table Count can be generated the MSSQL data sources. The following permissions are required the following MSSQL objects: select on sys.database_principals select on sys.database_permissions select on sys.database_role_members select on sys.sysdatabases select on sys.dm_exec_sessions select on sys.dm_exec_connections select on sys.server_principals select on sys.server_role_members These permissions are necessary the SQL queries in the fetch_report methods of the various classes within DatabaseRepo. The queries access system views and tables to retrieve user role privilege login and database information. These reports are explained in detail below. All User and Admin Count ¶ The report provides a summary of the number of admin and non-admin user accounts in an MSSQL database along with details about the data source and a timestamp indicating when the data was recorded. It is useful assessing the distribution of administrative and non-administrative users within the database. User Grants and Privileges ¶ The report provides information about the privileges and roles granted to various users within the master database including the type of grant and the timestamp when the data was recorded. It is useful auditing and managing user permissions and roles within the SQL database. Databases ¶ The report lists the names of various databases within a SQL Server instance. It is useful identifying and managing the different databases available on the server. User Privilege Report ¶ The report provides details about various users and roles within a SQL Server database including the number of roles granted user IDs and any associated role names. It is useful managing and auditing user permissions and roles in the database. â Login List ¶ The report logs the login events various users on an SQL Server detailing their status host information client address default database program used and data source details. It is useful monitoring and auditing user activities and connections to the server. User Granted Role and Table Count ¶ The report provides information about user accounts in a MSSQL database including the number of roles granted to each user and the data source details. It is useful tracking user privileges and understanding the structure of user roles within the database. â Back to top


================================================================================
SECTION 33: https://datatouch.netfein.com/latest/spectreports/mysql/
Content Length: 1,602 characters
================================================================================

MYSQL - Data Touch Skip to content MYSQL Data Source Spectrum Reports ¶ The reports All User and Admin Count User Privilege Report and User Granted Role and Table Count can be generated the MYSQL data sources. The following permissions are required the following MYSQL objects: select on mysql.user select on information_schema.USER_PRIVILEGES select on performance_schema.accounts select on performance_schema.host_cache These permissions are necessary the SQL queries in the fetch_report methods of the various classes within DatabaseRepo. The queries access system tables and views to retrieve user role privilege and login information. These reports are explained in detail below. All User and Admin Count ¶ The report provides a summary of the number of admin and non-admin user accounts in a MYSQL database along with details about the data source and a timestamp indicating when the data was recorded. It is useful assessing the distribution of administrative and non-administrative users within the database. User Privilege Report ¶ The report provides information about user accounts in a MySQL database including the number of roles granted to each user and the data source details. It is useful tracking user privileges and understanding the structure of user roles within the database. User Granted Role and Table Count ¶ The report provides information about user accounts in a MySQL database including the number of roles granted to each user and the data source details. It is useful tracking user privileges and understanding the structure of user roles within the database. Back to top


================================================================================
SECTION 34: https://datatouch.netfein.com/latest/spectreports/oracle/
Content Length: 4,031 characters
================================================================================

Oracle - Data Touch Skip to content ORACLE Data Source Spectrum Reports ¶ The reports Object Privileges User Privilege Report User Granted to Become User User Granted to Alter System All System Privileges and Admin Options Roles Granted User Granted Role and Table Count All Admin and User Roles All User and All User and Admin Count can be generated the Oracle data sources. The following permissions are required the following Oracle objects: select on dba_users select on all_tables select on dba_role_privs select on dba_sys_privs select on dba_roles select on dba_tab_privs select on all_users These permissions are necessary the SQL queries in the fetch_report methods of the various classes within DatabaseRepo . The queries access system views to retrieve user role privilege and table information. These reports are explained in detail below. Object Privileges ¶ The report provides information about user accounts in an Oracle database including the number of roles granted to each user and the data source details. It is useful tracking user privileges and understanding the structure of user roles within the database. User Privilege Report ¶ The report details a list of Oracle database users including their roles IDs account expiration dates group roles last login times creation dates table counts and a timestamp when the data was recorded. The report is useful managing user privileges and monitoring account activities within the database. User Granted to Become User ¶ The report lists Oracle database users grantees who have been granted the BECOME USER privilege indicating their ability to assume the identity of another user. Each entry includes details about the privilege whether it includes admin options the data source the timestamp of the data and a count of accounts with this privilege. â User Granted to Alter System ¶ The report lists Oracle database accounts with their respective privileges and the details of their data sources. It includes information about the type of privileges granted whether the admin option is enabled and the SQL risk timestamp. All System Privileges and Admin Options ¶ The report provides information on the system privileges assigned to various roles in an Oracle database. It includes details such as the grantee role the specific system privilege whether the privilege includes the admin option the data source details the timestamp of the data and the count of Oracle accounts with all system privileges and admin options. Roles Granted ¶ The report contains a list of authorizations and roles granted to specific users or roles in the Oracle database. Each row indicates a specific authorization granted to a user or role and the data source in which that authorization resides. User Granted Role and Table Count ¶ The report provides information about different Oracle database users including the number of roles granted to each user and the count of tables they have access to. It helps in understanding user privileges and the extent of their access within the database. â All Admin User Roles ¶ The report provides information about various Oracle database roles including the system privileges granted to each role whether they have admin options and their associated data source details. It helps in understanding the administrative capabilities and privileges assigned to different roles within the database. All User ¶ The report lists Oracle database users along with their data source details the timestamp of the SQL risk assessment and the count of Oracle accounts with full admin privileges. It provides insight into the administrative access and security status of various users within the database. â All User and Admin Count ¶ The report provides information about the number of admin and user accounts in an Oracle database along with details about the data source and a timestamp indicating when the data was recorded. It helps in assessing the distribution of administrative privileges and user accounts within the database. Back to top


================================================================================
SECTION 35: https://datatouch.netfein.com/latest/spectreports/postgres/
Content Length: 1,666 characters
================================================================================

PostgreSQL - Data Touch Skip to content PostgreSQL Data Source Spectrum Reports ¶ The reports All User and Admin Count User Privilege Report and User Granted Role and Table Count can be generated the PostgreSQL data sources. The following permissions are required the following PostgreSQL objects: select on pg_roles select on pg_class select on pg_namespace select on pg_auth_members select on pg_stat_activity select on pg_user These permissions are necessary the SQL queries in the fetch_report methods of the UserPrivilegeReport UserGrantedRoleAndTableCount and AllUserAndAdminCount classes. The queries access system catalogs and views to retrieve user role and table information. These reports are explained in detail below. All User and Admin Count ¶ The report provides a summary of the number of admin and non-admin user accounts in a PostgreSQL database along with details about the data source and a timestamp indicating when the data was recorded. It is useful assessing the distribution of administrative and non-administrative users within the database. User Privilege Report ¶ The report contains information about user accounts in a PostgreSQL database including their system IDs table counts group roles last login times and timestamps related to SQL risk assessments. It is useful managing and auditing database user activity and roles. User Granted Role and Table Count ¶ The report provides information about different PostgreSQL database users including the number of roles granted to each user and the count of tables they have access to. It helps in understanding user privileges and the extent of their access within the database. Back to top


================================================================================
SECTION 36: https://datatouch.netfein.com/latest/spectreports/saphana/
Content Length: 1,622 characters
================================================================================

SAP HANA - Data Touch Skip to content SAP HANA Data Source Spectrum Reports ¶ The reports All User and Admin Count User Privilege Report and User Granted Role and Table Count can be generated the SAP HANA data sources. The following permissions are required the following SAP HANA objects: select on sys.granted_privileges select on sys.users select on sys.granted_roles select on sys.tables These permissions are necessary the SQL queries in the fetch_report methods of the UserPrivilegeReport UserGrantedRoleAndTableCount and AllUserAndAdminCount classes. The queries access system catalogs and views to retrieve user role and table information. These reports are explained in detail below. All User and Admin Count ¶ The report provides a summary of the number of admin and non-admin user accounts in a SAP HANA database along with details about the data source and a timestamp indicating when the data was recorded. It is useful assessing the distribution of administrative and non-administrative users within the database. User Privilege Report ¶ The report contains information about user accounts in a SAP HANA database including their system IDs table counts group roles last login times and timestamps related to SQL risk assessments. It is useful managing and auditing database user activity and roles. User Granted Role and Table Count ¶ The report provides information about different SAP HANA database users including the number of roles granted to each user and the count of tables they have access to. It helps in understanding user privileges and the extent of their access within the database. Back to top


================================================================================
SECTION 37: https://datatouch.netfein.com/latest/spectreports/vertica/
Content Length: 1,294 characters
================================================================================

Vertica - Data Touch Skip to content Vertica Data Source Spectrum Reports ¶ The reports All User and Admin Count User Privilege Report and User Granted Role and Table Count** can be generated the Vertica data sources. The following permissions are required the following Vertica objects: select on v_catalog.users select on v_catalog.tables select on v_catalog.grants All User and Admin Count ¶ The report provides a summary of the number of admin and non-admin user accounts in a Vertica database along with details about the data source and a timestamp indicating when the data was recorded. It is useful assessing the distribution of administrative and non-administrative users within the database. User Privilege Report ¶ The report contains information about user accounts in a Vertica database including their system IDs table counts group roles last login times and timestamps related to SQL risk assessments. It is useful managing and auditing database user activity and roles. User Granted Role and Table Count ¶ The report provides information about different Vertica database users including the number of roles granted to each user and the count of tables they have access to. It helps in understanding user privileges and the extent of their access within the database. Back to top


================================================================================
SECTION 38: https://datatouch.netfein.com/latest/spectreports/couchbase/
Content Length: 1,024 characters
================================================================================

Couchbase - Data Touch Skip to content CouchBase Data Source Spectrum Reports ¶ The reports All User and Admin Count and User Privilege Report can be generated the CouchBase data sources. The following permissions are required the following CouchBase objects: REST API acces /settings/rbac/users and /settings/rbac/users/ username Query Select Query Manage permissions Security Admin or Read-Only Admin role All User and Admin Count ¶ The report provides a summary of the number of admin and non-admin user accounts in a CouchBase database along with details about the data source and a timestamp indicating when the data was recorded. It is useful assessing the distribution of administrative and non-administrative users within the database. User Privilege Report ¶ The report contains information about user accounts in a CouchBase database including their system IDs table counts group roles and timestamps related to SQL risk assessments. It is useful managing and auditing database user activity and roles. Back to top


================================================================================
SECTION 39: https://datatouch.netfein.com/latest/spectreports/elastic/
Content Length: 1,340 characters
================================================================================

Elasticsearch - Data Touch Skip to content Elasticsearch Data Source Spectrum Reports ¶ The reports All User and Admin Count User Privilege Report and User Granted Role and Table Count can be generated the Elasticsearch data sources. The following permissions are required the following Elasticsearch objects: select on system:user_info select on system:roles select on system:keyspaces REST API acces /_security/user/_privileges All User and Admin Count ¶ The report provides a summary of the number of admin and non-admin user accounts in a Elastic database along with details about the data source and a timestamp indicating when the data was recorded. It is useful assessing the distribution of administrative and non-administrative users within the database. User Privilege Report ¶ The report contains information about user accounts in an Elastic database including their system IDs table counts group roles and timestamps related to SQL risk assessments. It is useful managing and auditing database user activity and roles. User Granted Role and Table Count ¶ The report provides information about different Elastic database users including the number of roles granted to each user and the count of tables they have access to. It helps in understanding user privileges and the extent of their access within the database. Back to top


================================================================================
SECTION 40: https://datatouch.netfein.com/latest/relnotes/3.3.1/
Content Length: 267 characters
================================================================================

3.3.1 - Data Touch Skip to content 3.3.1 Software Version : 3.3.1 Publication Date : April 2025 What s New ¶ Introduced Health Check feature to monitor system status. Enhancements ¶ Improved the installation process a smoother and faster setup experience. Back to top


================================================================================
SECTION 41: https://datatouch.netfein.com/latest/relnotes/3.3.0/
Content Length: 939 characters
================================================================================

3.3.0 - Data Touch Skip to content 3.3.0 Software Version : 3.3.0 Publication Date : February 2025 What s New ¶ Introduced Integration feature that supports email functionality. Enhancements ¶ Spectrum reports now support Elasticsearch and Couchbase . Added Update Retention Period button to the Anomaly Detection - Scenario List page. Added Clone button to the Anomaly Detection - Scenario List page. Added Anomaly Detection - Scenario Templates to the Scenario Definition page predefined scenario definitions. Added Notify by Email and Retention Period options to the Spectrum Reports-Edit My Reports page. Added an Advanced Settings tab with Notify by Email and Retention Period options to the Spectrum Reports-New Report screen. Added a button to the Table and Column Lineage charts in the Data Lineage menu which allows navigating to the Client Usage chart. Improved the UI enhanced usability and a better user experience. Back to top


================================================================================
SECTION 42: https://datatouch.netfein.com/latest/relnotes/3.2.0/
Content Length: 650 characters
================================================================================

3.2.0 - Data Touch Skip to content 3.2.0 Software Version : 3.2.0 Publication Date : January 2025 Whatâs New ¶ Added Query Duration Reports Oracle MSSQL PostgreSQL and MySQL data sources. Added User Life Cycle feature. Added Product Information page. Added a setting to enable/disable source tracking . Enhancements ¶ Added parameter support reports. Added parameter support risk calculation. Spectrum reports now support Vertica . Updated the Client List page to organize data based on DB/OS Users. Integrated Data Touch with Redis high availability and token mechanism. Improved the UI enhanced usability and a better user experience. Back to top


================================================================================
SECTION 43: https://datatouch.netfein.com/latest/datacraft/datacraft/
Content Length: 316 characters
================================================================================

Overview - Data Touch Overview Data Craft is a utility tool designed to provide essential features commonly required in Proof of Concept PoC environments. It offers a set of functionalities to streamline testing and development processes. The available features and usage instructions are detailed below. Back to top


================================================================================
SECTION 44: https://datatouch.netfein.com/latest/datacraft/dcinstall/
Content Length: 992 characters
================================================================================

Installation - Data Touch Skip to content Data Craft Installation ¶ Before installing Data Craft Docker version 27 or higher must be installed on the system. 1. Import Docker Images ¶ Run the following script located in the packages directory to import the required Docker images: Bash ./load.sh 2. Start PostgreSQL Instance ¶ Use the following command to start the PostgreSQL container: Bash docker run -d --name = postgresql --restart = always --network = host -e POSTGRES_USER = myuser -e POSTGRES_PASSWORD = qxDpLDFo4do5U33q -e POSTGRES_DB = datatouch kafein/datacraft-postgresql:1.0.0 PostgreSQL Connection Details ¶ Host: IP address of the host machine Port: 25432 Username: myuser Password: qxDpLDFo4do5U33q Database: datatouch 3. Start Data Craft Services ¶ Navigate to the datacraft directory and run the following command: Bash docker-compose up -d Once the containers are up and running the Datacraft UI will be accessible at: Text Only http:// host-ip :3001/connection Back to top


================================================================================
SECTION 45: https://datatouch.netfein.com/latest/datacraft/dcusage/
Content Length: 9,258 characters
================================================================================

Usage - Data Touch Skip to content Data Craft Usage ¶ 1. Database Connection Initial Step ¶ Before using the tool configure your database connection. This is the required first step to enable access to all other functionalities. URL Required : Enter the database connection URL. For example: jdbc:postgresql:// IPaddressofhost : DBname Username Required : Enter the username used to access the database. In the example the username entered is: postgres Password Required : Enter the password used database access. The input is masked security reasons. Test Connection: Tests whether the database connection can be established with the provided information. Use this before saving to ensure the connection parameters are correct. Save: Saves the database connection details into the system. It is recommended to use this button after a successful connection test . All fields marked with an asterisk * are mandatory and must be filled in. This step is essential to use all other features of the Data Craft tool. If the connection information is incorrect data-related operations in the following steps will not work. 2. Data Generation ¶ Data Generation is the second step in the process and involves connecting to the DAM data source cleaning the target database and utilizing the data in DAM to create database structures and schemas. DAM Data Source Connection ¶ Enter the URL to connect to the DAM data source. Use Test Connection to verify the link. Use Save to store the connection settings. Cleaning Database Before Data Generation ¶ Truncate Tables Optional : Removes all previously generated database schema table column and sensitive type entries from SDM schemas. The results of this operation can be viewed in the Cleaned column of the Data Generation History table. Drop and Create Schemas Optional : Drops and recreates the schemas related to SDM. Select Discovered Database and Schema ¶ Select a Date Range . Based on the selected date range choose the target Databases and Schemas . Then click Execute to start the generation process. Data Generation History ¶ Displays a record of previously executed data generation tasks. Includes details such as schema name generation time and selected date range. Logs ¶ Displays logs on the right side of the page refreshing every 5 seconds. Shows detailed information about data generation such as: which tables received data field sizes any errors that occurred. Useful monitoring and troubleshooting. Sample Log Entries: ¶ Text Only schema name: PUBLIC table name: EMPLOYEE_TABLE saved field size: 5 schema name: PUBLIC table name: DEPARTMENT saved field size: 5 Key Functions ¶ Data Source Connection: A source database must be connected before generating any data. Database Cleaning Options: Choose to truncate existing data or drop and recreate schemas before new data generation. Schema and Database Selection: Select the target database and schema where data will be generated. Date Range Definition: Specify the time range which data should be created. Operational Logs: View detailed logs each operation including success and error messages. Mark Sensitive Data ¶ This page is used to identify sensitive data within the database and assign appropriate tokenization or masking methods . Note: The databases and schemas created in the first step are displayed on this screen. Select the relevant Database Schema Table and Column . Choose an appropriate Tokenization/Masking Type the sensitive data. Click the Save button to apply the configuration. You can track previous operations in the History section below. Screen Options ¶ Apply to All Tables: Applies the selected sensitive type to all columns of all tables in the selected schema. Apply to All Columns: Randomly assigns sensitive types to all columns of the selected table. Clear: Clears all previously selected actions and resets the form. Execute: Executes the sensitive type assignment based on selected parameters. History ¶ Displays a record of previously performed sensitive data tagging operations. Currently the table shows a No Data message. The History Table contains the following columns: Database Schema Semitoken Count Run Date Status and Action . Logs ¶ Displayed on the right side of the page and updated every 5 seconds. Logs include which tables were populated field sizes and any errors that occurred. This section is useful monitoring and troubleshooting sensitive data operations. Sensitive Details ¶ The Sensitive Details page shows all database tables and the number of columns within each table that are marked as containing sensitive data. It helps users to easily identify which tables include sensitive information and how widespread it is. Note: Users can click the Details button under the History section on the Mark Sensitive Data page to reach Sensitive Details . Through this page users can perform the actions listed below. View a high-level summary of the sensitive data distribution across tables. Identify tables that contain more sensitive fields. Prioritize certain tables data protection actions. On-Screen Sections ¶ 1. Table List ¶ Lists database tables such as: ACCOUNTS BRANCHES CONST_TABLE etc. 2. Column Information ¶ For each listed table two key metrics are displayed: Columns Count: Total number of columns in the table Sensitive Columns: Number of columns marked as sensitive 3. Example Data ¶ Some sample sensitive column distribution from the screen: ACCOUNTS : 3 out of 4 columns are sensitive BRANCHES : 3 out of 5 columns are sensitive CONST_TABLE : All 2 columns are sensitive This screen helps users gain insight into the scope of sensitive data across database tables making it easier to track and manage data security compliance. Create Audit Data ¶ Audit Data page displays the list of created queries initially shows a No Data message . Top-right buttons: Connections : Shortcut to connection management. New Query : Opens the query creation form. Logs Panel on the right shows query execution history and results initially empty . The process of creating audit data in the Data Craft application follows a structured sequence. Before creating scheduled queries database connections must be defined . Queries run based on these established connections. New Connection ¶ To define database connections enter the required fields: Name Database Type Host Port Database Name Username and Password . After entering connection details: Use Test Connection to check connectivity. Click Save to store the connection. A maximum of 5 users can be defined per database. A single connection can be used in up to 3 queries . Scheduled Queries ¶ Once a database connection is established scheduled queries can be created. New Query Page ¶ Used to define a scheduled query to generate audit data. Name : Name of the scheduled query. Description : Description of what the query does. Cron Expression : Defines how frequently the query will run e.g. every day at midnight . Database Type : Type of the database where the query will be executed selected from predefined connections. Users : List of database users who will execute the query. SQL Query : The SQL statement that will be executed on schedule. After filling in all details click Save to store the scheduled query. Important Conditions ¶ Connection First Then Query A database connection must exist before a scheduled query can be created. QueryâConnection Binding Queries are bound to pre-defined connections. The Database Type field in the query form pulls data from existing connections. Audit Data Generation Workflow Step 1: Define database connections in the Connections page. Step 2: Create scheduled queries in the Scheduled Queries page using the New Query button. Step 3: Queries run automatically at the scheduled times and generate audit logs. Step 4: Query results and logs appear in the Logs Panel . This process ensures that audit data is created systematically and automatically enabling efficient data governance and monitoring. Health Check ¶ Health check page is used to check the health status of the application through the Data Touch interface. Use this feature regularly to ensure that the system is functioning properly. Connection Parameters Top Section ¶ IP Address : IP address used to connect e.g. 192.168.1.1 Port : Port number e.g. 9098 Get Status : Starts the health check process Save URL : Saves the entered IP and port information Health Check Results Bottom Section ¶ Service : Name of the service e.g. KAFKA Status : Result of the health check e.g. SUCCESS Example Service Status List ¶ Process Status HEALTH_CHECK SUCCESS DATATOUCH SERVICE SUCCESS ANOMALY SERVICE SUCCESS REPORT SERVICE SUCCESS DATASOURCE SUCCESS RISK ENGINE SUCCESS Usage Purpose ¶ Monitor the live status of all Data Craft system components Quickly detect interruptions or failures in services Check operational status of critical infrastructure components like Kafka Provide actionable insights incident response Operational Flow ¶ Enter IP and Port information or use saved values Click Get Status to run the health check System evaluates the availability of all services Results are displayed in the Health Check View Status is shown as SUCCESS or an error message Click Save URL to store connection settings Back to top


================================================================================
SECTION 46: https://datatouch.netfein.com/latest/usage/libraryoverview/
Content Length: 284 characters
================================================================================

Overview - Data Touch Skip to content Welcome to the Data Touch Library ¶ This section provides access to all of our available documentation resources â both in PDF format and as video tutorials â to help you understand install and utilize Data Touch more effectively. Back to top
